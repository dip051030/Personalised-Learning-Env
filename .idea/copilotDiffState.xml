<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/db/loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/loader.py" />
              <option name="originalContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot; / &quot;lessons&quot;&#10;&#10;&#10;def load_lesson_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    path = DATA_DIR / filename&#10;    if not path.exists():&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;&#10;    return data&#10;" />
              <option name="updatedContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot; / &quot;lessons&quot;&#10;&#10;&#10;def load_lesson_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    path = DATA_DIR / filename&#10;    logging.info(f&quot;Loading lesson data from {path}&quot;)&#10;    if not path.exists():&#10;        logging.error(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;    logging.info(f&quot;Loaded {len(data)} lessons from {filename}&quot;)&#10;    return data" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/db/vector_db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/vector_db.py" />
              <option name="originalContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_lesson_data&#10;&#10;def build_chroma_db_collection(filename: str, collection_name: str = 'lessons'):&#10;    lessons = load_lesson_data(filename)&#10;    model = SentenceTransformer('all-MiniLM-L6-v2')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;)&#10;        }&#10;        &#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.Client()&#10;    collection = client.create_collection(name=collection_name)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas=metadatas&#10;    )&#10;    return collection, model&#10;" />
              <option name="updatedContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_lesson_data&#10;import logging&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;def build_chroma_db_collection(filename: str, collection_name: str = 'lessons'):&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_lesson_data(filename)&#10;    model = SentenceTransformer('all-MiniLM-L6-v2')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;)&#10;        }&#10;        &#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.Client()&#10;    collection = client.create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas=metadatas&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;    return collection, model" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/nodes.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/nodes.py" />
              <option name="originalContent" value="from langchain_core.messages import SystemMessage, HumanMessage&#10;from langgraph.graph import StateGraph, START, END&#10;from langsmith import expect&#10;from pydantic_core import ValidationError&#10;from sympy.stats import Expectation&#10;import logging&#10;&#10;from logis.logical_functions import decision_node, lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation&#10;from schemas import LearningState, ContentResponse&#10;import  json&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Process learning resource data.&#10;    &quot;&quot;&quot;&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved_content = retrieve_and_search(state=state)&#10;            logging.info(f&quot;Enriching content for resource: {retrieve_and_search(state=state)}&quot;)&#10;            response= enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.current_resource = state.current_resource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.current_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources' : state.current_resource.model_dump()&#10;            })&#10;            state.content_type = ContentResponse.LESSON if decision_node(state) == &quot;lesson_selection&quot; else ContentResponse.BLOG&#10;            state.next_action = response.content if hasattr(response, &quot;content&quot;) else response&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;&#10;            state.generated_content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;            logging.info(f&quot;Blog content has been generated!&quot;)&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = blog_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = blog_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;            logging.info(f&quot;Blog content has been generated!&quot;)&#10;            state.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;&#10;&#10;# def content_generation(state: LearningState) -&gt; LearningState:&#10;#     &quot;&quot;&quot;&#10;#     Generate content based on the current state.&#10;#     &quot;&quot;&quot;&#10;#     if state.user and state.current_resource:&#10;#         try:&#10;#             response= user_content_generation.invoke({&#10;#                 &quot;action&quot;: &quot;generate_content&quot;,&#10;#                 &quot;user_data&quot;: state.user.model_dump(),&#10;#                 &quot;resource_data&quot;: state.current_resource.model_dump()&#10;#             })&#10;#&#10;#             content_raw = response.content if hasattr(response, &quot;content&quot;) else response&#10;#             state.content = ContentResponse(content = content_raw)&#10;#             logging.info(f&quot;Content generated: {state.content}&quot;)&#10;#         except Expectation as e:&#10;#             logging.error(f&quot;Error generating content: {e}&quot;)&#10;#&#10;#     return state&#10;&#10;&#10;# def conent_improviser(state: LearningState) -&gt; LearningState:&#10;#     &quot;&quot;&quot;&#10;#     Improviser for content generation.&#10;#     &quot;&quot;&quot;&#10;#     if state.current_resource:&#10;#         try:&#10;#             response = user_content_generation.invoke({&#10;#                 &quot;action&quot;: &quot;improvise_content&quot;,&#10;#                 &quot;resource_data&quot;: state.generated_content.model_dump()&#10;#             })&#10;#&#10;#             improvised_content = response.content if hasattr(response, &quot;content&quot;) else response&#10;#             print('IMPROVISED CONTENT:', improvised_content)&#10;#         except ValidationError as e:&#10;#             print(f&quot;Error improvising content: {e}&quot;)&#10;#&#10;#     return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    if state.generated_content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_IMPROVISE_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;&#10;Unpolished Learning Resource:&#10;{state.generated_content.model_dump()}&#10;&quot;&quot;&quot;)&#10;            ]&#10;&#10;            response = content_improviser(messages)&#10;            generated_markdown = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            logging.info(f&quot;Improvised content: {generated_markdown}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: &quot;blog_generation&quot; if state.next_action == &quot;blog&quot; else &quot;content_generation&quot;,&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, END)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))&#10;" />
              <option name="updatedContent" value="from langchain_core.messages import SystemMessage, HumanMessage&#10;from langgraph.graph import StateGraph, START, END&#10;from langsmith import expect&#10;from pydantic_core import ValidationError&#10;from sympy.stats import Expectation&#10;import logging&#10;&#10;from logis.logical_functions import decision_node, lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation&#10;from schemas import LearningState, ContentResponse&#10;import  json&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering user_info_node&quot;)&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering enrich_content node&quot;)&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved_content = retrieve_and_search(state=state)&#10;            logging.info(f&quot;Retrieved content: {retrieved_content}&quot;)&#10;            response= enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.current_resource = state.current_resource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.current_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering route_selector_node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route for resource: {state.current_resource}&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources' : state.current_resource.model_dump()&#10;            })&#10;            state.content_type = ContentResponse.LESSON if decision_node(state) == &quot;lesson_selection&quot; else ContentResponse.BLOG&#10;            state.next_action = response.content if hasattr(response, &quot;content&quot;) else response&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_lesson_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;&#10;            state.generated_content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;            logging.info(f&quot;Lesson content has been generated!&quot;)&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_blog_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = blog_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for blog generation: {logical_response}&quot;)&#10;            response = blog_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;            state.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;            logging.info(f&quot;Blog content has been generated!&quot;)&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering content_improviser_node&quot;)&#10;    if state.generated_content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_IMPROVISE_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;&#10;Unpolished Learning Resource:&#10;{state.generated_content.model_dump()}&#10;&quot;&quot;&quot;)&#10;            ]&#10;&#10;            response = content_improviser(messages)&#10;            generated_markdown = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            logging.info(f&quot;Improvised content: {generated_markdown}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: &quot;blog_generation&quot; if state.next_action == &quot;blog&quot; else &quot;content_generation&quot;,&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, END)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/prompts/prompts.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/prompts/prompts.py" />
              <option name="originalContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse&#10;from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data:&#10;{existing_data}&#10;&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    A simplified prompt template for enriching a learning resource.&#10;    The model is expected to:&#10;    - Expand and clarify fields like 'description' and 'elaboration'&#10;    - Generate summaries and student-friendly explanations&#10;    - Maintain the same keys as input (structured JSON output)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a curriculum enrichment agent. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields (like `description`, `elaboration`)&#10;- Add a student-friendly summary&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;current_resources_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resource_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Converts the provided topic data dictionary into a JSON-formatted&#10;        string to insert into the prompt.&#10;        &quot;&quot;&quot;&#10;        return self.format(&#10;            current_resource_data=json.dumps(current_resource_data, indent=2)&#10;        )&#10;&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;&#10;    The model must return ONLY the markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and dont introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict) -&gt; str:&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;&#10;from langchain.prompts import PromptTemplate&#10;import json&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        super.__init__(&#10;            template = '''&#10;            You are a route selector for an educational learning system.&#10;            Your task is to determine the next action based on the user''s current state and progress.&#10;            Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;&#10;    def format_prompt(self, action: str, current_resources: dict) -&gt; str:&#10;        return self.format(&#10;            action=action,&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(LearningResource)&#10;# user_content_generation = prompt_content_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichContent)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelectorNode)&#10;content_generation = prompt_content_generation | get_gemini_model(LearningResource)&#10;blog_generation = prompt_blog_generation | get_gemini_model(LearningResource)&#10;content_improviser = get_groq_model()" />
              <option name="updatedContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;import logging&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse&#10;from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing UserSummaryTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data:&#10;{existing_data}&#10;&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(f&quot;Formatting UserSummaryTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    A simplified prompt template for enriching a learning resource.&#10;    The model is expected to:&#10;    - Expand and clarify fields like 'description' and 'elaboration'&#10;    - Generate summaries and student-friendly explanations&#10;    - Maintain the same keys as input (structured JSON output)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing EnrichContent Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a curriculum enrichment agent. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields (like `description`, `elaboration`)&#10;- Add a student-friendly summary&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;current_resources_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resource_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Converts the provided topic data dictionary into a JSON-formatted&#10;        string to insert into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(&quot;Formatting EnrichContent prompt&quot;)&#10;        return self.format(&#10;            current_resource_data=json.dumps(current_resource_data, indent=2)&#10;        )&#10;&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;&#10;    The model must return ONLY the markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing ContentGenerationTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and dont introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting ContentGenerationTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;&#10;from langchain.prompts import PromptTemplate&#10;import json&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing BlogGenerationPrompt Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting BlogGenerationPrompt for style: {style}&quot;)&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing RouteSelectorNode Template&quot;)&#10;        super.__init__(&#10;            template = '''&#10;            You are a route selector for an educational learning system.&#10;            Your task is to determine the next action based on the user''s current state and progress.&#10;            Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, current_resources: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting RouteSelectorNode prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(LearningResource)&#10;# user_content_generation = prompt_content_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichContent)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelectorNode)&#10;content_generation = prompt_content_generation | get_gemini_model(LearningResource)&#10;blog_generation = prompt_blog_generation | get_gemini_model(LearningResource)&#10;content_improviser = get_groq_model()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>