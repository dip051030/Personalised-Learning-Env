<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/db/loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/loader.py" />
              <option name="originalContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot;&#10;&#10;&#10;def load_json_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;&#10;    Load lesson data from a JSON file in the lessons data directory.&#10;&#10;    Args:&#10;        filename (str): The name of the lesson data file.&#10;&#10;    Returns:&#10;        List[Dict[str, Any]]: List of lesson data dictionaries.&#10;&#10;    Raises:&#10;        FileNotFoundError: If the file does not exist in the data directory.&#10;    &quot;&quot;&quot;&#10;    path = DATA_DIR / filename&#10;    logging.info(f&quot;Loading lesson data from {path}&quot;)&#10;    if not path.exists():&#10;        logging.error(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;    logging.info(f&quot;Loaded {len(data)} lessons from {filename}&quot;)&#10;    return data&#10;" />
              <option name="updatedContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot;&#10;&#10;&#10;def load_json_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;&#10;    Load lesson data from a JSON file in the lessons data directory.&#10;&#10;    Args:&#10;        filename (str): The name of the lesson data file.&#10;&#10;    Returns:&#10;        List[Dict[str, Any]]: List of lesson data dictionaries.&#10;&#10;    Raises:&#10;        FileNotFoundError: If the file does not exist in the data directory.&#10;    &quot;&quot;&quot;&#10;    path = DATA_DIR / filename&#10;    try:&#10;        logging.info(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] INFO Loading lesson data from {path}&quot;)&#10;        if not path.exists():&#10;            logging.error(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] ERROR {filename} not found in {DATA_DIR}&quot;)&#10;            raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;            data = json.load(f)&#10;        logging.info(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] INFO Loaded {len(data)} lessons from {filename}&quot;)&#10;        return data&#10;    except Exception as e:&#10;        logging.error(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] ERROR Failed to load lesson data: {e}&quot;)&#10;        return []" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/db/vector_db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/vector_db.py" />
              <option name="originalContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Vector database utilities for building and saving ChromaDB collections from lesson and scraped data.&#10;&quot;&quot;&quot;&#10;import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Cleaned metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/logis/logical_functions.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/logis/logical_functions.py" />
              <option name="originalContent" value="import logging&#10;import chromadb&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack, ContentType&#10;from db.vector_db import build_chroma_db_collection, save_scraped_data_to_vdb&#10;from sentence_transformers import SentenceTransformer&#10;&#10;&#10;def search_both_collections(state : LearningState,&#10;                            vdb_path=&quot;./local VDB/chromadb&quot;,&#10;                            lessons_collection=&quot;lessons&quot;,&#10;                            scraped_collection=&quot;scraped_data&quot;,&#10;                            n_results=1):&#10;    &quot;&quot;&quot;&#10;    Search both the lessons and scraped_data collections for the most similar items to the query.&#10;    Returns results from both collections.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is None:&#10;            return None&#10;&#10;&#10;        build_chroma_db_collection()&#10;        save_scraped_data_to_vdb()&#10;        # Load the embedding model&#10;        model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;        query_text = state.current_resource.topic&#10;        query_embedding = model.encode(query_text).tolist()&#10;&#10;        # Connect to ChromaDB&#10;        client = chromadb.PersistentClient(path=vdb_path)&#10;        lessons_col = client.get_or_create_collection(lessons_collection)&#10;        scraped_col = client.get_or_create_collection(scraped_collection)&#10;&#10;        # Query both collections&#10;        lessons_results = lessons_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;        scraped_results = scraped_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;&#10;        return {&#10;            &quot;lessons_results&quot;: lessons_results,&#10;            &quot;scraped_results&quot;: scraped_results&#10;        }&#10;    except Exception as e:&#10;        logging.error(f&quot;Error searching collections: {e}&quot;)&#10;        return None&#10;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on curriculum metadata, topic type, and phrasing.&#10;    Returns a string indicating the lesson style.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic.lower()&#10;    unit = state.current_resource.unit.lower()&#10;    desc = state.current_resource.description.lower()&#10;&#10;    if &quot;evaluation&quot; in unit:&#10;        style = &quot;evaluation_component&quot;&#10;    elif &quot;practical&quot; in state.current_resource.topic_id or &quot;activity&quot; in topic or &quot;experiment&quot; in desc:&#10;        style = &quot;experimental&quot;&#10;    elif any(keyword in topic for keyword in [&quot;derive&quot;, &quot;calculate&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;formula&quot;]):&#10;        style = &quot;problem_solving&quot;&#10;    elif any(keyword in desc for keyword in [&quot;used in&quot;, &quot;applied in&quot;, &quot;application&quot;, &quot;real-world&quot;]):&#10;        style = &quot;application_based&quot;&#10;    elif &quot;revision&quot; in topic or &quot;summary&quot; in topic:&#10;        style = &quot;revision_summary&quot;&#10;    elif &quot;quiz&quot; in topic or state.content_type == ContentType.QUIZ:&#10;        style = &quot;interactive_quiz&quot;&#10;    elif &quot;enrich&quot; in topic or &quot;context&quot; in desc:&#10;        style = &quot;enrichment&quot;&#10;    else:&#10;        style = &quot;conceptual_focus&quot;&#10;&#10;    return style&#10;&#10;&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    Returns a LearningResource instance.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata.get('subject', 'unknown').lower()),&#10;        grade=metadata.get(&quot;grade&quot;),&#10;        unit=metadata.get(&quot;unit&quot;),&#10;        topic_id=metadata.get(&quot;topic_id&quot;),&#10;        topic=metadata.get(&quot;topic_title&quot;),&#10;        description=metadata.get(&quot;description&quot;, &quot;&quot;),&#10;        keywords=metadata.get(&quot;keywords&quot;).split(&quot;,&quot;),&#10;        hours=metadata.get(&quot;hours&quot;),&#10;        references=metadata.get(&quot;references&quot;),&#10;        elaboration=metadata.get(&quot;elaboration&quot;, &quot;&quot;)&#10;    )&#10;&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    Returns a string indicating the blog style.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style&#10;&#10;def update_content_count(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Check the content count in the learning state.&#10;    Returns a string indicating if an update is required.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.count &lt; 4:&#10;            logging.info(f&quot;Current state count: {state.count}&quot;)&#10;            return 'Update required'&#10;        else:&#10;            return 'No update required'&#10;    except Exception as e:&#10;        logging.error(f&quot;Error updating state count: {e}&quot;)&#10;        return 'No update required'" />
              <option name="updatedContent" value="import logging&#10;import chromadb&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack, ContentType&#10;from db.vector_db import build_chroma_db_collection, save_scraped_data_to_vdb&#10;from sentence_transformers import SentenceTransformer&#10;&#10;&#10;def search_both_collections(state : LearningState,&#10;                            vdb_path=&quot;./local VDB/chromadb&quot;,&#10;                            lessons_collection=&quot;lessons&quot;,&#10;                            scraped_collection=&quot;scraped_data&quot;,&#10;                            n_results=1):&#10;    &quot;&quot;&quot;&#10;    Search both the lessons and scraped_data collections for the most similar items to the query.&#10;    Returns results from both collections.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is None:&#10;            logging.warning(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] WARNING No current_resource in state.&quot;)&#10;            return None&#10;&#10;&#10;        build_chroma_db_collection()&#10;        save_scraped_data_to_vdb()&#10;        # Load the embedding model&#10;        model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;        query_text = state.current_resource.topic&#10;        query_embedding = model.encode(query_text).tolist()&#10;&#10;        # Connect to ChromaDB&#10;        client = chromadb.PersistentClient(path=vdb_path)&#10;        lessons_col = client.get_or_create_collection(lessons_collection)&#10;        scraped_col = client.get_or_create_collection(scraped_collection)&#10;&#10;        # Query both collections&#10;        lessons_results = lessons_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;        scraped_results = scraped_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;&#10;        logging.info(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] INFO Queried both collections for topic '{query_text}'.&quot;)&#10;        return {&#10;            &quot;lessons_results&quot;: lessons_results,&#10;            &quot;scraped_results&quot;: scraped_results&#10;        }&#10;    except Exception as e:&#10;        logging.error(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] ERROR Error searching collections: {e}&quot;)&#10;        return None&#10;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on curriculum metadata, topic type, and phrasing.&#10;    Returns a string indicating the lesson style.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic.lower()&#10;    unit = state.current_resource.unit.lower()&#10;    desc = state.current_resource.description.lower()&#10;&#10;    if &quot;evaluation&quot; in unit:&#10;        style = &quot;evaluation_component&quot;&#10;    elif &quot;practical&quot; in state.current_resource.topic_id or &quot;activity&quot; in topic or &quot;experiment&quot; in desc:&#10;        style = &quot;experimental&quot;&#10;    elif any(keyword in topic for keyword in [&quot;derive&quot;, &quot;calculate&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;formula&quot;]):&#10;        style = &quot;problem_solving&quot;&#10;    elif any(keyword in desc for keyword in [&quot;used in&quot;, &quot;applied in&quot;, &quot;application&quot;, &quot;real-world&quot;]):&#10;        style = &quot;application_based&quot;&#10;    elif &quot;revision&quot; in topic or &quot;summary&quot; in topic:&#10;        style = &quot;revision_summary&quot;&#10;    elif &quot;quiz&quot; in topic or state.content_type == ContentType.QUIZ:&#10;        style = &quot;interactive_quiz&quot;&#10;    elif &quot;enrich&quot; in topic or &quot;context&quot; in desc:&#10;        style = &quot;enrichment&quot;&#10;    else:&#10;        style = &quot;conceptual_focus&quot;&#10;&#10;    return style&#10;&#10;&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    Returns a LearningResource instance.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata.get('subject', 'unknown').lower()),&#10;        grade=metadata.get(&quot;grade&quot;),&#10;        unit=metadata.get(&quot;unit&quot;),&#10;        topic_id=metadata.get(&quot;topic_id&quot;),&#10;        topic=metadata.get(&quot;topic_title&quot;),&#10;        description=metadata.get(&quot;description&quot;, &quot;&quot;),&#10;        keywords=metadata.get(&quot;keywords&quot;).split(&quot;,&quot;),&#10;        hours=metadata.get(&quot;hours&quot;),&#10;        references=metadata.get(&quot;references&quot;),&#10;        elaboration=metadata.get(&quot;elaboration&quot;, &quot;&quot;)&#10;    )&#10;&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    Returns a string indicating the blog style.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style&#10;&#10;def update_content_count(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Check the content count in the learning state.&#10;    Returns a string indicating if an update is required.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.count &lt; 4:&#10;            logging.info(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] INFO Current state count: {state.count}&quot;)&#10;            return 'Update required'&#10;        else:&#10;            logging.info(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] INFO No update required, current count: {state.count}&quot;)&#10;            return 'No update required'&#10;    except Exception as e:&#10;        logging.error(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] ERROR Error updating state count: {e}&quot;)&#10;        return 'No update required'" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/external_tools_apis.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/external_tools_apis.py" />
              <option name="originalContent" value="import requests&#10;import logging&#10;from keys.apis import set_env&#10;&#10;&#10;def serp_api_tool(query: str) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Use SerpAPI to search for the given query and return the results.&#10;    Returns a dictionary with the search results.&#10;    &quot;&quot;&quot;&#10;    data = {}&#10;&#10;    try:&#10;        api_key = set_env('SERP_API_KEY')&#10;        if not api_key:&#10;            raise ValueError(&quot;SERP_API_KEY is not set. Please set it in your environment variables.&quot;)&#10;&#10;        headers = {&#10;            'X-API-KEY': api_key,&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        params = {&#10;            'q': query + ' site:byjus.com OR site:vedantu.com OR site:toppr.com OR site:learnfatafat.com',&#10;            'engine': &quot;google&quot;,&#10;            'num': 10,&#10;            'gl': 'in'&#10;&#10;        }&#10;&#10;        response = requests.post('https://google.serper.dev/search', json=params, headers=headers)&#10;        data = response.json()&#10;&#10;    except Exception as e:&#10;        logging.error(f&quot;Error in SerpAPI tool: {e}&quot;)&#10;        data = {&quot;error&quot;: str(e)}&#10;&#10;    return data" />
              <option name="updatedContent" value="import requests&#10;import logging&#10;from keys.apis import set_env&#10;&#10;&#10;def serp_api_tool(query: str) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Use SerpAPI to search for the given query and return the results.&#10;    Returns a dictionary with the search results.&#10;    &quot;&quot;&quot;&#10;    data = {}&#10;    try:&#10;        api_key = set_env('SERP_API_KEY')&#10;        if not api_key:&#10;            raise ValueError(&quot;SERP_API_KEY is not set. Please set it in your environment variables.&quot;)&#10;        headers = {&#10;            'X-API-KEY': api_key,&#10;            'Content-Type': 'application/json'&#10;        }&#10;        params = {&#10;            'q': query + ' site:byjus.com OR site:vedantu.com OR site:toppr.com OR site:learnfatafat.com',&#10;            'engine': &quot;google&quot;,&#10;            'num': 10,&#10;            'gl': 'in'&#10;        }&#10;        response = requests.post('https://google.serper.dev/search', json=params, headers=headers)&#10;        data = response.json()&#10;        logging.info(f&quot;[external_tools_apis.py:{serp_api_tool.__code__.co_firstlineno}] INFO SerpAPI request successful for query: {query}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;[external_tools_apis.py:{serp_api_tool.__code__.co_firstlineno}] ERROR in SerpAPI tool: {e}&quot;)&#10;        data = {&quot;error&quot;: str(e)}&#10;    return data" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scrapper/crawl4ai_scrapping.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scrapper/crawl4ai_scrapping.py" />
              <option name="originalContent" value="import logging&#10;from crawl4ai import (&#10;    AsyncWebCrawler,&#10;    BrowserConfig,&#10;    CrawlerRunConfig,&#10;    LLMExtractionStrategy,&#10;    LLMConfig, CacheMode&#10;)&#10;from datetime import datetime&#10;import json&#10;from keys.apis import set_env&#10;from schemas import WebCrawlerConfig&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;async def crawl_and_extract_json(urls: list) -&gt; list:&#10;&#10;    browser_cfg = BrowserConfig(&#10;        browser_type=&quot;firefox&quot;,&#10;        headless=False,&#10;        verbose=True,&#10;        light_mode=False&#10;    )&#10;&#10;&#10;&#10;    api_token = set_env('GROQ_DEEPSEEK_API_KEY')&#10;    if not api_token:&#10;        raise ValueError(&quot;Environment variable 'GROQ_DEEPSEEK_API_KEY' is not set or invalid.&quot;)&#10;&#10;    llm_cfg = LLMConfig(&#10;        provider='ollama/llama3',&#10;        temperature=0,&#10;    )&#10;&#10;    extraction_strategy = LLMExtractionStrategy(&#10;        llm_config=llm_cfg,&#10;        instruction=&quot;&quot;&quot;&#10;        You are an intelligent educational content extractor. You must output a clean, valid JSON object **exactly** following the schema and rules below. If data is not present on the page, return empty lists or nulls — but you must still return a full valid JSON object.&#10;&#10;        DO NOT skip the output. DO NOT return natural language. DO NOT explain anything.&#10;&#10;        ---&#10;&#10;        STRICT JSON Output Schema:&#10;        {&#10;          &quot;url&quot;: &quot;&lt;original URL&gt;&quot;,  // always required&#10;          &quot;title&quot;: &quot;&lt;main article H1 title or null&gt;&quot;,  &#10;          &quot;headings&quot;: [&quot;&lt;H2–H4 section/sub-section headings&gt;&quot;],  &#10;          &quot;main_findings&quot;: [&quot;&lt;key educational facts, definitions, or concepts in full sentences&gt;&quot;],  &#10;          &quot;content&quot;: &quot;&lt;single combined block of all main findings in original order or null if none&gt;&quot;,  &#10;          &quot;keywords&quot;: [&quot;&lt;list of core nouns or terms from headings and findings or []&gt;&quot;]&#10;        }&#10;&#10;        ---&#10;&#10;        Rules:&#10;&#10;        Must Include:&#10;        - Main article title (`&lt;h1&gt;` tag or first major visible title)&#10;        - All `&lt;h2&gt;`, `&lt;h3&gt;`, and `&lt;h4&gt;` tags from the **main content body only**&#10;        - Individual full-sentence educational statements — laws, definitions, examples, formulas, etc.&#10;        - Bullet points **only if** each is a full sentence with factual educational info&#10;        - Join all findings in `content` (preserve exact sentence order and original phrasing)&#10;        - Extract `keywords` by identifying repeated or important terms in `headings` + `main_findings`&#10;&#10;        Must Exclude:&#10;        - Website name, branding, menus, footers, navbars, cookie banners, popups, forms, ads&#10;        - Code blocks, timestamps, author bios, comments, vague filler, layout-only text&#10;        - External links, social media, UI elements, inline styles, HTML/CSS/JS&#10;&#10;        ---&#10;&#10;        Output Guidelines:&#10;        - If any field is missing from the page, return:&#10;          - `null` for title or content&#10;          - `[]` for headings, main_findings, or keywords&#10;        - The `&quot;url&quot;` field is always required and must match the input&#10;        - Output must be **strictly valid JSON** — no markdown, no commentary, no extra text&#10;&#10;        ---&#10;&#10;        YOUR ROLE:&#10;        You are a **non-generative extractor** — not a writer. Never summarize, paraphrase, or invent.&#10;        Your goal is to extract clean, structured data for curriculum systems and AI tutors.&#10;&#10;        ALWAYS return full JSON — even if data is minimal.&#10;        &quot;&quot;&quot;&#10;        ,&#10;    input_format='markdown',&#10;        schema=WebCrawlerConfig.model_json_schema()&#10;    )&#10;&#10;    crawl_cfg = CrawlerRunConfig(&#10;        magic=True,&#10;        excluded_tags=[&#10;            'footer',&#10;            'nav',&#10;            'aside',&#10;            'script',&#10;            'style',&#10;            'link',&#10;            'form',&#10;            'noscript',&#10;            'iframe',&#10;            'svg',&#10;            'canvas',&#10;            'input',&#10;            'button',&#10;            'select',&#10;            'option',&#10;            'label',&#10;            'object',&#10;            'embed',&#10;            'video',&#10;            'audio'&#10;        ],&#10;        excluded_selector=&#10;            '.ads, .advertisement, .sponsored, .promo, .sidebar, .related-links, .comments, .comment, '&#10;            '.social-links, .share-buttons, .social-media, .footer, .footer-links, .footer-info, '&#10;            '.footer-text, .footer-logo, .footer-social, .footer-contact, .footer-legal, .footer-privacy, '&#10;            '.footer-terms, .footer-copyright, .footer-disclaimer, .footer-sitemap, '&#10;            '.footer-subscribe, .footer-newsletter, .footer-contact-form, .footer-address, '&#10;            '.footer-menu, .cookie-notice, .cookie-banner, .cookies, .popup, .modal, '&#10;            '.popup-overlay, .popup-content, .popup-close, .popup-header, .popup-body, '&#10;            '.popup-footer, .popup-buttons, .popup-link, .login, .signup, .login-form, '&#10;            '.register, .auth, .nav, .navbar, .navigation, .menu, .topbar, .toolbar, '&#10;            '.header, .masthead, .banner, .cta, .newsletter, .subscribe, .sticky, '&#10;            '.chatbot, .livechat, .intercom-launcher, .notifications, .alert, .announcement, '&#10;            '.breadcrumb, .pagination, .loader, .loading, .spinner, .hero, .widget, .widget-area, '&#10;            '.search-box, .search-form, .search-bar, .scroll-to-top, .back-to-top, .branding, '&#10;            '.related-posts, .related-articles, .more-articles, .external-links, .print-button',&#10;        only_text = True,&#10;        remove_forms=True,&#10;        exclude_external_links=True,&#10;        exclude_social_media_links=True,&#10;        verbose=True,&#10;        extraction_strategy=extraction_strategy,&#10;    )&#10;&#10;    results = []&#10;    async with AsyncWebCrawler(config=browser_cfg) as crawler:&#10;        for url in urls:&#10;            try:&#10;                logging.info(f&quot;Crawling -&gt; {url}&quot;)&#10;&#10;                result = await crawler.arun(&#10;                    url=url,&#10;                    config=crawl_cfg&#10;                )&#10;&#10;                if result.success:&#10;                    logging.info(f&quot;Successfully crawled {url}&quot;)&#10;&#10;                    extracted_list = json.loads(result.extracted_content)&#10;&#10;                    extracted = extracted_list[0] if isinstance(extracted_list, list) else extracted_list&#10;&#10;                    results.append({&#10;                        &quot;url&quot;: extracted.get(&quot;url&quot;, url),&#10;                        &quot;source&quot;: extracted.get(&quot;source&quot;, &quot;&quot;),  # e.g. &quot;byjus.com&quot;&#10;                        &quot;subject&quot;: extracted.get(&quot;subject&quot;, &quot;&quot;),  # e.g. &quot;Physics&quot;&#10;                        &quot;grade&quot;: extracted.get(&quot;grade&quot;, None),  # e.g. 11 (int) or None&#10;                        &quot;unit&quot;: extracted.get(&quot;unit&quot;, &quot;&quot;),  # e.g. &quot;Electricity and Magnetism&quot;&#10;                        &quot;topic_title&quot;: extracted.get(&quot;topic_title&quot;, None),  # optional, e.g. &quot;Coulomb’s law&quot;&#10;                        &quot;title&quot;: extracted.get(&quot;title&quot;),&#10;                        &quot;headings&quot;: extracted.get(&quot;headings&quot;, []),&#10;                        &quot;main_findings&quot;: extracted.get(&quot;main_findings&quot;, []),&#10;                        &quot;content&quot;: extracted.get(&quot;content&quot;),&#10;                        &quot;keywords&quot;: extracted.get(&quot;keywords&quot;, []),  # new field for keywords&#10;                        &quot;word_count&quot;: extracted.get(&#10;                            &quot;word_count&quot;,&#10;                            len(&quot; &quot;.join(extracted.get(&quot;main_findings&quot;, [])).split())&#10;                        ),&#10;                        &quot;status&quot;: &quot;success&quot;,&#10;                        &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                    })&#10;                    extraction_strategy.show_usage()&#10;&#10;            except Exception as e:&#10;                logging.error(f&quot;Error crawling {url}: {e}&quot;)&#10;                results.append({&#10;                    &quot;url&quot;: url,&#10;                    &quot;source&quot;: &quot;&quot;,&#10;                    &quot;subject&quot;: &quot;&quot;,&#10;                    &quot;grade&quot;: None,&#10;                    &quot;unit&quot;: &quot;&quot;,&#10;                    &quot;topic_title&quot;: None,&#10;                    &quot;title&quot;: None,&#10;                    &quot;headings&quot;: [],&#10;                    &quot;main_findings&quot;: [],&#10;                    &quot;content&quot;: None,&#10;                    &quot;keywords&quot;: [],&#10;                    &quot;word_count&quot;: 0,&#10;                    &quot;status&quot;: &quot;failed&quot;,&#10;                    &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                })&#10;&#10;    return results" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Educational content crawling and extraction utilities using crawl4ai and LLM strategies.&#10;&quot;&quot;&quot;&#10;import logging&#10;from crawl4ai import (&#10;    AsyncWebCrawler,&#10;    BrowserConfig,&#10;    CrawlerRunConfig,&#10;    LLMExtractionStrategy,&#10;    LLMConfig, CacheMode&#10;)&#10;from datetime import datetime&#10;import json&#10;from keys.apis import set_env&#10;from schemas import WebCrawlerConfig&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;async def crawl_and_extract_json(urls: list) -&gt; list:&#10;    &quot;&quot;&quot;&#10;    Crawl a list of URLs and extract educational content as JSON objects.&#10;    Args:&#10;        urls (list): List of URLs to crawl.&#10;    Returns:&#10;        list: List of extracted JSON objects for each URL.&#10;    &quot;&quot;&quot;&#10;    browser_cfg = BrowserConfig(&#10;        browser_type=&quot;firefox&quot;,&#10;        headless=False,&#10;        verbose=True,&#10;        light_mode=False&#10;    )&#10;&#10;    api_token = set_env('GROQ_DEEPSEEK_API_KEY')&#10;    if not api_token:&#10;        raise ValueError(&quot;Environment variable 'GROQ_DEEPSEEK_API_KEY' is not set or invalid.&quot;)&#10;&#10;    llm_cfg = LLMConfig(&#10;        provider='ollama/llama3',&#10;        temperature=0,&#10;    )&#10;&#10;    extraction_strategy = LLMExtractionStrategy(&#10;        llm_config=llm_cfg,&#10;        instruction=&quot;&quot;&quot;&#10;        You are an intelligent educational content extractor. You must output a clean, valid JSON object **exactly** following the schema and rules below. If data is not present on the page, return empty lists or nulls — but you must still return a full valid JSON object.&#10;&#10;        DO NOT skip the output. DO NOT return natural language. DO NOT explain anything.&#10;&#10;        ---&#10;&#10;        STRICT JSON Output Schema:&#10;        {&#10;          &quot;url&quot;: &quot;&lt;original URL&gt;&quot;,  // always required&#10;          &quot;title&quot;: &quot;&lt;main article H1 title or null&gt;&quot;,  &#10;          &quot;headings&quot;: [&quot;&lt;H2–H4 section/sub-section headings&gt;&quot;],  &#10;          &quot;main_findings&quot;: [&quot;&lt;key educational facts, definitions, or concepts in full sentences&gt;&quot;],  &#10;          &quot;content&quot;: &quot;&lt;single combined block of all main findings in original order or null if none&gt;&quot;,  &#10;          &quot;keywords&quot;: [&quot;&lt;list of core nouns or terms from headings and findings or []&gt;&quot;]&#10;        }&#10;&#10;        ---&#10;&#10;        Rules:&#10;&#10;        Must Include:&#10;        - Main article title (`&lt;h1&gt;` tag or first major visible title)&#10;        - All `&lt;h2&gt;`, `&lt;h3&gt;`, and `&lt;h4&gt;` tags from the **main content body only**&#10;        - Individual full-sentence educational statements — laws, definitions, examples, formulas, etc.&#10;        - Bullet points **only if** each is a full sentence with factual educational info&#10;        - Join all findings in `content` (preserve exact sentence order and original phrasing)&#10;        - Extract `keywords` by identifying repeated or important terms in `headings` + `main_findings`&#10;&#10;        Must Exclude:&#10;        - Website name, branding, menus, footers, navbars, cookie banners, popups, forms, ads&#10;        - Code blocks, timestamps, author bios, comments, vague filler, layout-only text&#10;        - External links, social media, UI elements, inline styles, HTML/CSS/JS&#10;&#10;        ---&#10;&#10;        Output Guidelines:&#10;        - If any field is missing from the page, return:&#10;          - `null` for title or content&#10;          - `[]` for headings, main_findings, or keywords&#10;        - The `&quot;url&quot;` field is always required and must match the input&#10;        - Output must be **strictly valid JSON** — no markdown, no commentary, no extra text&#10;&#10;        ---&#10;&#10;        YOUR ROLE:&#10;        You are a **non-generative extractor** — not a writer. Never summarize, paraphrase, or invent.&#10;        Your goal is to extract clean, structured data for curriculum systems and AI tutors.&#10;&#10;        ALWAYS return full JSON — even if data is minimal.&#10;        &quot;&quot;&quot;&#10;        ,&#10;    input_format='markdown',&#10;        schema=WebCrawlerConfig.model_json_schema()&#10;    )&#10;&#10;    crawl_cfg = CrawlerRunConfig(&#10;        magic=True,&#10;        excluded_tags=[&#10;            'footer',&#10;            'nav',&#10;            'aside',&#10;            'script',&#10;            'style',&#10;            'link',&#10;            'form',&#10;            'noscript',&#10;            'iframe',&#10;            'svg',&#10;            'canvas',&#10;            'input',&#10;            'button',&#10;            'select',&#10;            'option',&#10;            'label',&#10;            'object',&#10;            'embed',&#10;            'video',&#10;            'audio'&#10;        ],&#10;        excluded_selector=&#10;            '.ads, .advertisement, .sponsored, .promo, .sidebar, .related-links, .comments, .comment, '&#10;            '.social-links, .share-buttons, .social-media, .footer, .footer-links, .footer-info, '&#10;            '.footer-text, .footer-logo, .footer-social, .footer-contact, .footer-legal, .footer-privacy, '&#10;            '.footer-terms, .footer-copyright, .footer-disclaimer, .footer-sitemap, '&#10;            '.footer-subscribe, .footer-newsletter, .footer-contact-form, .footer-address, '&#10;            '.footer-menu, .cookie-notice, .cookie-banner, .cookies, .popup, .modal, '&#10;            '.popup-overlay, .popup-content, .popup-close, .popup-header, .popup-body, '&#10;            '.popup-footer, .popup-buttons, .popup-link, .login, .signup, .login-form, '&#10;            '.register, .auth, .nav, .navbar, .navigation, .menu, .topbar, .toolbar, '&#10;            '.header, .masthead, .banner, .cta, .newsletter, .subscribe, .sticky, '&#10;            '.chatbot, .livechat, .intercom-launcher, .notifications, .alert, .announcement, '&#10;            '.breadcrumb, .pagination, .loader, .loading, .spinner, .hero, .widget, .widget-area, '&#10;            '.search-box, .search-form, .search-bar, .scroll-to-top, .back-to-top, .branding, '&#10;            '.related-posts, .related-articles, .more-articles, .external-links, .print-button',&#10;        only_text = True,&#10;        remove_forms=True,&#10;        exclude_external_links=True,&#10;        exclude_social_media_links=True,&#10;        verbose=True,&#10;        extraction_strategy=extraction_strategy,&#10;    )&#10;&#10;    results = []&#10;    async with AsyncWebCrawler(config=browser_cfg) as crawler:&#10;        for url in urls:&#10;            try:&#10;                logging.info(f&quot;Crawling -&gt; {url}&quot;)&#10;&#10;                result = await crawler.arun(&#10;                    url=url,&#10;                    config=crawl_cfg&#10;                )&#10;&#10;                if result.success:&#10;                    logging.info(f&quot;Successfully crawled {url}&quot;)&#10;&#10;                    extracted_list = json.loads(result.extracted_content)&#10;&#10;                    extracted = extracted_list[0] if isinstance(extracted_list, list) else extracted_list&#10;&#10;                    results.append({&#10;                        &quot;url&quot;: extracted.get(&quot;url&quot;, url),&#10;                        &quot;source&quot;: extracted.get(&quot;source&quot;, &quot;&quot;),  # e.g. &quot;byjus.com&quot;&#10;                        &quot;subject&quot;: extracted.get(&quot;subject&quot;, &quot;&quot;),  # e.g. &quot;Physics&quot;&#10;                        &quot;grade&quot;: extracted.get(&quot;grade&quot;, None),  # e.g. 11 (int) or None&#10;                        &quot;unit&quot;: extracted.get(&quot;unit&quot;, &quot;&quot;),  # e.g. &quot;Electricity and Magnetism&quot;&#10;                        &quot;topic_title&quot;: extracted.get(&quot;topic_title&quot;, None),  # optional, e.g. &quot;Coulomb’s law&quot;&#10;                        &quot;title&quot;: extracted.get(&quot;title&quot;),&#10;                        &quot;headings&quot;: extracted.get(&quot;headings&quot;, []),&#10;                        &quot;main_findings&quot;: extracted.get(&quot;main_findings&quot;, []),&#10;                        &quot;content&quot;: extracted.get(&quot;content&quot;),&#10;                        &quot;keywords&quot;: extracted.get(&quot;keywords&quot;, []),  # new field for keywords&#10;                        &quot;word_count&quot;: extracted.get(&#10;                            &quot;word_count&quot;,&#10;                            len(&quot; &quot;.join(extracted.get(&quot;main_findings&quot;, [])).split())&#10;                        ),&#10;                        &quot;status&quot;: &quot;success&quot;,&#10;                        &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                    })&#10;                    extraction_strategy.show_usage()&#10;&#10;            except Exception as e:&#10;                logging.error(f&quot;Error crawling {url}: {e}&quot;)&#10;                results.append({&#10;                    &quot;url&quot;: url,&#10;                    &quot;source&quot;: &quot;&quot;,&#10;                    &quot;subject&quot;: &quot;&quot;,&#10;                    &quot;grade&quot;: None,&#10;                    &quot;unit&quot;: &quot;&quot;,&#10;                    &quot;topic_title&quot;: None,&#10;                    &quot;title&quot;: None,&#10;                    &quot;headings&quot;: [],&#10;                    &quot;main_findings&quot;: [],&#10;                    &quot;content&quot;: None,&#10;                    &quot;keywords&quot;: [],&#10;                    &quot;word_count&quot;: 0,&#10;                    &quot;status&quot;: &quot;failed&quot;,&#10;                    &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                })&#10;&#10;    return results" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scrapper/save_to_local.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scrapper/save_to_local.py" />
              <option name="originalContent" value="import json&#10;from typing import Union&#10;&#10;from models.external_tools_apis import serp_api_tool&#10;from schemas import LearningState&#10;&#10;def serper_api_results_parser(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Parses the results from the SerpAPI tool based on the current learning state.&#10;&#10;    Args:&#10;        state (LearningState): The current learning state containing the topic and grade.&#10;&#10;    Returns:&#10;        dict: The search results retrieved from the SerpAPI tool.&#10;    &quot;&quot;&quot;&#10;    serpapi_search_results = serp_api_tool(&#10;        query=state.current_resource.topic + 'for grade ' + str(state.current_resource.grade))&#10;    return serpapi_search_results&#10;&#10;&#10;def save_to_local(data: Union[dict, list], file_path: str):&#10;    &quot;&quot;&quot;&#10;    Saves the provided data (dict or list) to a local JSON file.&#10;&#10;    Args:&#10;        data (dict or list): The data to be saved.&#10;        file_path (str): The path where the data will be saved.&#10;&#10;    Raises:&#10;        TypeError: If the data contains unsupported types for JSON serialization.&#10;    &quot;&quot;&quot;&#10;    if not isinstance(data, (dict, list)):&#10;        raise TypeError(&quot;Data must be a dict or a list to be saved as JSON.&quot;)&#10;    with open(file_path, mode='w', encoding='utf-8') as f:&#10;        json.dump(data, f, indent=4, ensure_ascii=False)" />
              <option name="updatedContent" value="import json&#10;import logging&#10;import os&#10;from typing import Union&#10;&#10;from models.external_tools_apis import serp_api_tool&#10;from schemas import LearningState&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def serper_api_results_parser(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Parses the results from the SerpAPI tool based on the current learning state.&#10;&#10;    Args:&#10;        state (LearningState): The current learning state containing the topic and grade.&#10;&#10;    Returns:&#10;        dict: The search results retrieved from the SerpAPI tool.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        serpapi_search_results = serp_api_tool(&#10;            query=state.current_resource.topic + ' for grade ' + str(state.current_resource.grade))&#10;        logging.info(f&quot;[save_to_local.py:{serper_api_results_parser.__code__.co_firstlineno}] INFO SerpAPI results parsed for topic '{state.current_resource.topic}' and grade '{state.current_resource.grade}'&quot;)&#10;        return serpapi_search_results&#10;    except Exception as e:&#10;        logging.error(f&quot;[save_to_local.py:{serper_api_results_parser.__code__.co_firstlineno}] ERROR Failed to parse SerpAPI results: {e}&quot;)&#10;        return {}&#10;&#10;def save_to_local(data: Union[dict, list], file_path: str):&#10;    &quot;&quot;&quot;&#10;    Saves the provided data (dict or list) to a local JSON file.&#10;&#10;    Args:&#10;        data (dict or list): The data to be saved.&#10;        file_path (str): The path where the data will be saved.&#10;&#10;    Raises:&#10;        TypeError: If the data contains unsupported types for JSON serialization.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if not isinstance(data, (dict, list)):&#10;            raise TypeError(&quot;Data must be a dict or a list to be saved as JSON.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, mode='w', encoding='utf-8') as f:&#10;            json.dump(data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;[save_to_local.py:{save_to_local.__code__.co_firstlineno}] INFO Data saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;[save_to_local.py:{save_to_local.__code__.co_firstlineno}] ERROR Failed to save data to {file_path}: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils.py" />
              <option name="updatedContent" value="import os&#10;import json&#10;import logging&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/utils.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Utility functions for saving learning state and generated content to files.&#10;&quot;&quot;&quot;&#10;import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Always use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save generated content to {file_path}: {e}&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Utility functions for saving learning state and generated content to files.&#10;&quot;&quot;&quot;&#10;import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Always use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;[utils.py:{save_learning_state_to_json.__code__.co_firstlineno}] INFO LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;[utils.py:{save_learning_state_to_json.__code__.co_firstlineno}] ERROR Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;[utils.py:{save_generated_content.__code__.co_firstlineno}] INFO Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;[utils.py:{save_generated_content.__code__.co_firstlineno}] ERROR Failed to save generated content to {file_path}: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>