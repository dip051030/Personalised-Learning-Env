<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="import json&#10;import logging&#10;from nodes import graph_run&#10;from utils.utils import save_learning_state_to_json&#10;&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;user_data = {&#10;    &quot;user&quot;: {&#10;        &quot;username&quot;: &quot;student_01&quot;,&#10;        &quot;age&quot;: 17,&#10;        &quot;grade&quot;: 12,&#10;        &quot;id&quot;: 101,&#10;        &quot;is_active&quot;: True,&#10;        &quot;user_info&quot;: &quot;A motivated grade 12 student interested in physics.&quot;&#10;    },&#10;    &quot;current_resource&quot;: {&#10;        &quot;subject&quot;: &quot;physics&quot;,&#10;        &quot;grade&quot;: 12,&#10;        &quot;unit&quot;: &quot;Mechanics&quot;,&#10;        &quot;topic_id&quot;: &quot;6.4&quot;,&#10;        &quot;topic&quot;: &quot;Centripetal force&quot;,&#10;        &quot;description&quot;: &quot;Define and calculate centripetal force for systems like satellites in orbit or vehicles on curved roads.&quot;,&#10;        &quot;elaboration&quot;: &quot;Define and calculate centripetal force for systems like satellites in orbit or vehicles on curved roads.&quot;,&#10;        &quot;keywords&quot;: [&quot;centripetal force&quot;, &quot;circular motion&quot;, &quot;satellite orbits&quot;],&#10;        &quot;hours&quot;: 9,&#10;        &quot;references&quot;: &quot;Page 46&quot;&#10;    },&#10;    &quot;progress&quot;: [],&#10;    &quot;next_action&quot;: {&quot;next_node&quot;: &quot;lesson_blog&quot;},&#10;    &quot;history&quot;: [],&#10;    &quot;enriched_resource&quot;: None,&#10;    &quot;topic_data&quot;: None,&#10;    &quot;related_examples&quot;: None,&#10;    &quot;content_type&quot;: &quot;lesson&quot;,&#10;    &quot;content&quot;: None,&#10;    &quot;feedback&quot;: None&#10;}&#10;&#10;def main():&#10;    &quot;&quot;&quot;&#10;    Entry point for running the learning graph with sample user data.&#10;    &quot;&quot;&quot;&#10;    output = graph_run(user_data)&#10;    logging.info(f&quot;Graph has given an output! {output}&quot;)&#10;    # Save the learning state to a JSON file&#10;    save_learning_state_to_json(output, &quot;learning_state.json&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="import json&#10;import logging&#10;from nodes import graph_run&#10;from utils.utils import save_learning_state_to_json&#10;&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;user_data = {&#10;    &quot;user&quot;: {&#10;        &quot;username&quot;: &quot;student_01&quot;,&#10;        &quot;age&quot;: 17,&#10;        &quot;grade&quot;: 12,&#10;        &quot;id&quot;: 101,&#10;        &quot;is_active&quot;: True,&#10;        &quot;user_info&quot;: &quot;A motivated grade 12 student interested in physics.&quot;&#10;    },&#10;    &quot;current_resource&quot;: {&#10;        &quot;subject&quot;: &quot;physics&quot;,&#10;        &quot;grade&quot;: 12,&#10;        &quot;unit&quot;: &quot;Mechanics&quot;,&#10;        &quot;topic_id&quot;: &quot;6.4&quot;,&#10;        &quot;topic&quot;: &quot;Centripetal force&quot;,&#10;        &quot;description&quot;: &quot;Define and calculate centripetal force for systems like satellites in orbit or vehicles on curved roads.&quot;,&#10;        &quot;elaboration&quot;: &quot;Define and calculate centripetal force for systems like satellites in orbit or vehicles on curved roads.&quot;,&#10;        &quot;keywords&quot;: [&quot;centripetal force&quot;, &quot;circular motion&quot;, &quot;satellite orbits&quot;],&#10;        &quot;hours&quot;: 9,&#10;        &quot;references&quot;: &quot;Page 46&quot;&#10;    },&#10;    &quot;progress&quot;: [],&#10;    &quot;next_action&quot;: {&quot;next_node&quot;: &quot;lesson_blog&quot;},&#10;    &quot;history&quot;: [],&#10;    &quot;enriched_resource&quot;: None,&#10;    &quot;topic_data&quot;: None,&#10;    &quot;related_examples&quot;: None,&#10;    &quot;content_type&quot;: &quot;lesson&quot;,&#10;    &quot;content&quot;: None,&#10;    &quot;feedback&quot;: None&#10;}&#10;&#10;def main():&#10;    &quot;&quot;&quot;&#10;    Entry point for running the learning graph with sample user data.&#10;    &quot;&quot;&quot;&#10;    output = graph_run(user_data)&#10;    logging.info(f&quot;Graph has given an output! {output}&quot;)&#10;    # Save the learning state to a JSON file&#10;    save_learning_state_to_json(output, &quot;learning_state.json&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/nodes.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/nodes.py" />
              <option name="originalContent" value="from more_itertools import flatten&#10;from langgraph.graph import StateGraph, END&#10;from langchain_core.messages import HumanMessage&#10;&#10;from logis.logical_functions import lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search, update_content_count&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation, \&#10;    CONTENT_FEEDBACK_SYSTEM_PROMPT, prompt_content_improviser, prompt_feedback, content_feedback, gap_finder&#10;from schemas import LearningState, ContentResponse, EnrichedLearningResource, FeedBack, RouteSelector&#10;import json&#10;import logging&#10;import os&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to process and summarize user information using the user_summary prompt.&#10;    Updates the state with validated user info.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering user_info_node&quot;)&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;            print(response)&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            print('hello')&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to enrich the current learning resource using LLM enrichment.&#10;    Updates the state with an enriched resource.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering enrich_content node&quot;)&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved = retrieve_and_search(state=state)&#10;            if not retrieved:&#10;                logging.error(&quot;No content retrieved from vector DB.&quot;)&#10;                return state&#10;            retrieved_content = retrieved.get('metadatas', [])&#10;            if not retrieved_content:&#10;                logging.error(&quot;No metadatas found in retrieved content.&quot;)&#10;                return state&#10;            retrieved_content = list(flatten(retrieved_content))[0]&#10;            print('RETRIEVED CONTENT', retrieved_content)&#10;            response = enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.enriched_resource = EnrichedLearningResource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.enriched_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to select the next route (lesson or blog) based on the enriched resource.&#10;    Updates the state with the next action.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering route_selector_node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route for resource: {state.current_resource}&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources': state.enriched_resource.model_dump()&#10;            })&#10;            # Set next_action as a RouteSelector model&#10;            next_action_str = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.next_action = RouteSelector(next_node=next_action_str)&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to generate lesson content using the content_generation prompt.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering generate_lesson_content node&quot;)&#10;    if state.user is not None and state.enriched_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.enriched_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            # print(f'Generated Content: {resource_data}')&#10;            state.content = ContentResponse(content = resource_data)&#10;            logging.info(f&quot;Lesson content has been generated!&quot;)&#10;            print(f'Generated Content: {state.content}')&#10;        except Exception as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to generate blog content using the blog_generation prompt.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering generate_blog_content node&quot;)&#10;    if state.user is not None and state.enriched_resource is not None:&#10;        try:&#10;            logical_response = blog_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for blog generation: {logical_response}&quot;)&#10;            response = blog_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.enriched_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.content = ContentResponse(content=resource_data)&#10;            logging.info(f&quot;Blog content has been generated!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to improve generated content using the content improver LLM.&#10;    Uses the latest feedback (including gaps) to improve the content.&#10;    Updates the state with improved content.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering content_improviser_node&quot;)&#10;    if state.content is not None and state.feedback is not None:&#10;        try:&#10;            messages = [&#10;                prompt_content_improviser,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.model_dump()}&#10;&#10;Please improve the content by making it more engaging, informative, and suitable for the target audience.&#10;&#10;Feedback (including gaps):&#10;{state.feedback.model_dump()}&#10;&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = content_improviser.invoke(messages)&#10;            improved_content = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            # Update state.content with the newly generated improvised content&#10;            state.content = ContentResponse(content=improved_content)&#10;            logging.info(f&quot;Improvised content has been generated and updated in state.content!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def collect_feedback_node(state:LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to collect feedback on generated content.&#10;    Always uses the latest content and updates state.feedback with new feedback.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering collect_feedback_node&quot;)&#10;    feedback_data = None&#10;    if state.content is not None:&#10;        try:&#10;            logging.info(&quot;Collecting feedback for content&quot;)&#10;            messages = [&#10;                prompt_feedback,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.content}&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = content_feedback.invoke(messages)&#10;            logging.info(&quot;Feedback has been collected!&quot;)&#10;            feedback_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            feedback_data = json.loads(feedback_data) if isinstance(feedback_data, str) else feedback_data&#10;            state.feedback = FeedBack.model_validate(feedback_data)&#10;            logging.info(f&quot;Feedback processed and updated: {state.feedback}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error collecting feedback: {e}&quot;)&#10;    return state&#10;&#10;&#10;def find_content_gap_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to find content gaps based on user feedback.&#10;    Updates the feedback in state with new gaps for the next improvise node.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering find_content_gap_node&quot;)&#10;    if state.feedback is not None and state.content is not None:&#10;        logging.info(f&quot;Finding content gaps based on feedback: {state.feedback}&quot;)&#10;        data = gap_finder.invoke({&#10;            'content': state.content.content if hasattr(state.content, 'content') else str(state.content),&#10;            'feedback': state.feedback.model_dump(),&#10;        })&#10;        response = data.content if hasattr(data, &quot;content&quot;) else data&#10;        print(f'Gaps : {response}')&#10;        # Update feedback with new gaps for the next improvise node&#10;        updated_feedback = FeedBack.model_validate(json.loads(response) if isinstance(response, str) else response)&#10;        state.feedback = updated_feedback&#10;        logging.info(f&quot;Feedback received and updated: {state.feedback}&quot;)&#10;    return state&#10;&#10;def update_state(state: LearningState) -&gt; LearningState:&#10;    try:&#10;        if not hasattr(state, &quot;count&quot;):&#10;            state.count = 0&#10;        response = update_content_count(state)&#10;        if response == 'Update required':&#10;            state.count += 1&#10;            logging.info(f&quot;State updated: {state.count}&quot;)&#10;        else:&#10;            logging.info(f&quot;No update required, current count: {state.count}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Error updating state: {e}&quot;)&#10;    return state&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;builder.add_node(&quot;collect_feedback&quot;, collect_feedback_node)&#10;builder.add_node(&quot;find_content_gap&quot;, find_content_gap_node)&#10;builder.add_node(&quot;update_state&quot;, update_state)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: (&#10;        state.next_action.next_node&#10;        if hasattr(state.next_action, &quot;next_node&quot;) and state.next_action.next_node in [&quot;blog_generation&quot;, &quot;content_generation&quot;]&#10;        else &quot;content_generation&quot;  # Default branch if next_action is missing or invalid&#10;    ),&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, 'collect_feedback')&#10;builder.add_edge(&quot;collect_feedback&quot;, &quot;find_content_gap&quot;)&#10;builder.add_edge(&quot;find_content_gap&quot;, &quot;update_state&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;update_state&quot;,&#10;    lambda state: &quot;content_improviser&quot; if getattr(state, &quot;count&quot;, 0) &lt; 4 else &quot;END&quot;,&#10;    {&#10;        &quot;content_improviser&quot;: &quot;content_improviser&quot;,&#10;        &quot;END&quot;: END&#10;    }&#10;)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))&#10;" />
              <option name="updatedContent" value="from more_itertools import flatten&#10;from langgraph.graph import StateGraph, END&#10;from langchain_core.messages import HumanMessage&#10;&#10;from logis.logical_functions import lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search, update_content_count&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation, \&#10;    CONTENT_FEEDBACK_SYSTEM_PROMPT, prompt_content_improviser, prompt_feedback, content_feedback, gap_finder&#10;from schemas import LearningState, ContentResponse, EnrichedLearningResource, FeedBack, RouteSelector&#10;import json&#10;import logging&#10;import os&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to process and summarize user information using the user_summary prompt.&#10;    Updates the state with validated user info.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering user_info_node&quot;)&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;            print(response)&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            print('hello')&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to enrich the current learning resource using LLM enrichment.&#10;    Updates the state with an enriched resource.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering enrich_content node&quot;)&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved = retrieve_and_search(state=state)&#10;            if not retrieved:&#10;                logging.error(&quot;No content retrieved from vector DB.&quot;)&#10;                return state&#10;            retrieved_content = retrieved.get('metadatas', [])&#10;            if not retrieved_content:&#10;                logging.error(&quot;No metadatas found in retrieved content.&quot;)&#10;                return state&#10;            retrieved_content = list(flatten(retrieved_content))[0]&#10;            print('RETRIEVED CONTENT', retrieved_content)&#10;            response = enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.enriched_resource = EnrichedLearningResource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.enriched_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to select the next route (lesson or blog) based on the enriched resource.&#10;    Updates the state with the next action.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering route_selector_node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route for resource: {state.current_resource}&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources': state.enriched_resource.model_dump()&#10;            })&#10;            # Set next_action as a RouteSelector model&#10;            next_action_str = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.next_action = RouteSelector(next_node=next_action_str)&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to generate lesson content using the content_generation prompt.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering generate_lesson_content node&quot;)&#10;    if state.user is not None and state.enriched_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.enriched_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            # print(f'Generated Content: {resource_data}')&#10;            state.content = ContentResponse(content = resource_data)&#10;            logging.info(f&quot;Lesson content has been generated!&quot;)&#10;            print(f'Generated Content: {state.content}')&#10;        except Exception as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to generate blog content using the blog_generation prompt.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering generate_blog_content node&quot;)&#10;    if state.user is not None and state.enriched_resource is not None:&#10;        try:&#10;            logical_response = blog_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for blog generation: {logical_response}&quot;)&#10;            response = blog_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.enriched_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.content = ContentResponse(content=resource_data)&#10;            logging.info(f&quot;Blog content has been generated!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to improve generated content using the content improver LLM.&#10;    Uses the latest feedback (including gaps) to improve the content.&#10;    Updates the state with improved content.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering content_improviser_node&quot;)&#10;    if state.content is not None and state.feedback is not None:&#10;        try:&#10;            messages = [&#10;                prompt_content_improviser,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.model_dump()}&#10;&#10;Please improve the content by making it more engaging, informative, and suitable for the target audience.&#10;&#10;Feedback (including gaps):&#10;{state.feedback.model_dump()}&#10;&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = content_improviser.invoke(messages)&#10;            improved_content = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            # Update state.content with the newly generated improvised content&#10;            state.content = ContentResponse(content=improved_content)&#10;            logging.info(f&quot;Improvised content has been generated and updated in state.content!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def collect_feedback_node(state:LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to collect feedback on generated content.&#10;    Always uses the latest content and updates state.feedback with new feedback.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering collect_feedback_node&quot;)&#10;    feedback_data = None&#10;    if state.content is not None:&#10;        try:&#10;            logging.info(&quot;Collecting feedback for content&quot;)&#10;            messages = [&#10;                prompt_feedback,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.content}&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = content_feedback.invoke(messages)&#10;            logging.info(&quot;Feedback has been collected!&quot;)&#10;            feedback_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            feedback_data = json.loads(feedback_data) if isinstance(feedback_data, str) else feedback_data&#10;            state.feedback = FeedBack.model_validate(feedback_data)&#10;            logging.info(f&quot;Feedback processed and updated: {state.feedback}&quot;)&#10;            # Log rating and gaps for debugging&#10;            logging.info(f&quot;Feedback rating: {state.feedback.rating}, gaps: {state.feedback.gaps}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error collecting feedback: {e}&quot;)&#10;    return state&#10;&#10;&#10;def find_content_gap_node(state: LearningState) -&gt; LearningState:&#10;    &quot;&quot;&quot;&#10;    Node to find content gaps based on user feedback.&#10;    Updates the feedback in state with new gaps for the next improvise node.&#10;    &quot;&quot;&quot;&#10;    logging.info(&quot;Entering find_content_gap_node&quot;)&#10;    if state.feedback is not None and state.content is not None:&#10;        logging.info(f&quot;Finding content gaps based on feedback: {state.feedback}&quot;)&#10;        data = gap_finder.invoke({&#10;            'content': state.content.content if hasattr(state.content, 'content') else str(state.content),&#10;            'feedback': state.feedback.model_dump(),&#10;        })&#10;        response = data.content if hasattr(data, &quot;content&quot;) else data&#10;        print(f'Gaps : {response}')&#10;        # Update feedback with new gaps for the next improvise node&#10;        updated_feedback = FeedBack.model_validate(json.loads(response) if isinstance(response, str) else response)&#10;        state.feedback = updated_feedback&#10;        logging.info(f&quot;Feedback received and updated: {state.feedback}&quot;)&#10;        # Log rating and gaps for debugging&#10;        logging.info(f&quot;GapFinder rating: {state.feedback.rating}, gaps: {state.feedback.gaps}&quot;)&#10;    return state&#10;&#10;def update_state(state: LearningState) -&gt; LearningState:&#10;    try:&#10;        if not hasattr(state, &quot;count&quot;):&#10;            state.count = 0&#10;        response = update_content_count(state)&#10;        if response == 'Update required':&#10;            state.count += 1&#10;            logging.info(f&quot;State updated: {state.count}&quot;)&#10;        else:&#10;            logging.info(f&quot;No update required, current count: {state.count}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Error updating state: {e}&quot;)&#10;    return state&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;builder.add_node(&quot;collect_feedback&quot;, collect_feedback_node)&#10;builder.add_node(&quot;find_content_gap&quot;, find_content_gap_node)&#10;builder.add_node(&quot;update_state&quot;, update_state)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: (&#10;        state.next_action.next_node&#10;        if hasattr(state.next_action, &quot;next_node&quot;) and state.next_action.next_node in [&quot;blog_generation&quot;, &quot;content_generation&quot;]&#10;        else &quot;content_generation&quot;  # Default branch if next_action is missing or invalid&#10;    ),&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, 'collect_feedback')&#10;builder.add_edge(&quot;collect_feedback&quot;, &quot;find_content_gap&quot;)&#10;builder.add_edge(&quot;find_content_gap&quot;, &quot;update_state&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;update_state&quot;,&#10;    lambda state: &quot;content_improviser&quot; if getattr(state, &quot;count&quot;, 0) &lt; 4 else &quot;END&quot;,&#10;    {&#10;        &quot;content_improviser&quot;: &quot;content_improviser&quot;,&#10;        &quot;END&quot;: END&#10;    }&#10;)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/prompts/prompts.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/prompts/prompts.py" />
              <option name="originalContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model, get_deepseek_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse, EnrichedLearningResource, RouteSelector, FeedBack&#10;&#10;&#10;# from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, *args, **kwargs):&#10;        logging.info(&quot;Initializing UserSummaryTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data: {existing_data}&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;], *args, **kwargs&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(f&quot;Formatting UserSummaryTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=&quot;&quot;&quot;You're a curriculum enrichment agent, your job is {action}. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields.&#10;- Add a student-friendly but formal content enrichment to the fields.&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;,&#10;            input_variables=[&quot;current_resources_data&quot;, &quot;action&quot;]&#10;        )&#10;        logging.info(&quot;Initializing EnrichContent Template&quot;)&#10;&#10;    def format_prompt(self, action: str, current_resources_data: dict) -&gt; str:&#10;        logging.info(&quot;Formatting EnrichContent prompt&quot;)&#10;        return self.format(&#10;            action=action,&#10;            current_resources_data=json.dumps(current_resources_data, indent=2)&#10;        )&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;    The model must return ONLY the Markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing ContentGenerationTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Style:&#10;{style}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and don't introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&#10;IMPORTANT: Output ONLY the markdown lesson content. Do NOT include any explanations, JSON, or extra text before or after the markdown.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting ContentGenerationTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing BlogGenerationPrompt Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting BlogGenerationPrompt for style: {style}&quot;)&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing RouteSelectorNode Template&quot;)&#10;        super().__init__(&#10;            template = '''&#10;You are a route selector for an educational learning system.&#10;Your task is to determine the next action based on the user''s current state and progress.&#10;Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resources: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting RouteSelectorNode prompt for action.&quot;)&#10;        return self.format(&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_FEEDBACK_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an intelligent feedback assistant trained to process and structure user feedback on educational content.&#10;&#10;Your goal is to analyze the provided content and comments, and generate a clean JSON object with the following fields ONLY:&#10;&#10;- `rating`: An integer from 1 to 5 (1 = very poor, 5 = excellent).&#10;- `comments`: A short criticised summary of the content.&#10;- `needed`: A boolean indicating if feedback is needed (True) or not (False).&#10;- `gaps`: A list of specific content gaps or areas for improvement (optional, can be empty).&#10;&#10;**Instructions:**&#10;- Return ONLY a valid JSON object with these four fields.&#10;- Do NOT include any extra text, explanation, or fields.&#10;- If a field is not available, still include it with a reasonable default (e.g., `comments` can be an empty string, `gaps` can be an empty list).&#10;- Never invent new fields or explanations.&#10;- Your output must be a single JSON object, nothing else.&#10;&#10;Example output:&#10;{&#10;  &quot;rating&quot;: 4,&#10;  &quot;comments&quot;: &quot;The explanation was clear and engaging, but could use more real-world examples.&quot;,&#10;  &quot;needed&quot;: true,&#10;  &quot;gaps&quot;: [&#10;    &quot;Missing real-world applications of the concept.&quot;,&#10;    &quot;Could include more visual aids.&quot;&#10;  ]&#10;}&#10;&quot;&quot;&quot;)&#10;&#10;class ContentGapGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to identify content gaps in educational material based on feedback and the content itself.&#10;    The LLM should return a JSON object matching the FeedBack class, with:&#10;      - rating: integer (1-5)&#10;      - comments: summary of feedback and identified gaps&#10;      - needed: boolean (True if improvement is needed)&#10;      - gaps: list of specific content gaps (optional, for next model)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You are an expert educational content reviewer.&#10;&#10;Your task is to analyze the following learning content and the feedback provided, and identify any content gaps, missing explanations, unclear sections, or areas for improvement.&#10;&#10;CONTENT:&#10;{content}&#10;&#10;FEEDBACK:&#10;{feedback}&#10;&#10;Instructions:&#10;- Carefully read both the content and the feedback.&#10;- Identify and list all content gaps, missing details, or unclear explanations.&#10;- If the feedback already mentions gaps, include them. If you find additional gaps, add those too.&#10;- Return a JSON object with the following fields ONLY:&#10;  - &quot;rating&quot;: integer from 1 to 5 (overall quality)&#10;  - &quot;comments&quot;: a short summary of the main feedback and gaps&#10;  - &quot;needed&quot;: true if improvement is needed, false if not&#10;  - &quot;gaps&quot;: a list of specific content gaps or improvement points&#10;&#10;Example output:&#10;{{&#10;  &quot;rating&quot;: 3,&#10;  &quot;comments&quot;: &quot;The content is generally clear but lacks real-world examples and visual aids. Some sections are too brief.&quot;,&#10;  &quot;needed&quot;: true,&#10;  &quot;gaps&quot;: [&#10;    &quot;No real-world examples provided.&quot;,&#10;    &quot;Missing diagrams or visual explanations.&quot;,&#10;    &quot;The explanation of the formula derivation is too brief.&quot;&#10;  ]&#10;}}&#10;- Do NOT include any extra text or explanation outside the JSON.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;content&quot;, &quot;feedback&quot;]&#10;        )&#10;&#10;    def format_prompt(self, content: str, feedback: dict) -&gt; str:&#10;        return self.format(&#10;            content=content,&#10;            feedback=feedback&#10;        )&#10;&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_feedback = CONTENT_FEEDBACK_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;prompt_gap_finder = ContentGapGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichedLearningResource)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelector)&#10;content_generation = prompt_content_generation | get_gemini_model(ContentResponse)&#10;blog_generation = prompt_blog_generation | get_gemini_model(ContentResponse)&#10;gap_finder = prompt_gap_finder | get_gemini_model(FeedBack)&#10;content_improviser =get_groq_model()&#10;content_feedback = get_deepseek_model(FeedBack)&#10;" />
              <option name="updatedContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model, get_deepseek_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse, EnrichedLearningResource, RouteSelector, FeedBack&#10;&#10;&#10;# from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, *args, **kwargs):&#10;        logging.info(&quot;Initializing UserSummaryTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data: {existing_data}&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;], *args, **kwargs&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(f&quot;Formatting UserSummaryTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=&quot;&quot;&quot;You're a curriculum enrichment agent, your job is {action}. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields.&#10;- Add a student-friendly but formal content enrichment to the fields.&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;,&#10;            input_variables=[&quot;current_resources_data&quot;, &quot;action&quot;]&#10;        )&#10;        logging.info(&quot;Initializing EnrichContent Template&quot;)&#10;&#10;    def format_prompt(self, action: str, current_resources_data: dict) -&gt; str:&#10;        logging.info(&quot;Formatting EnrichContent prompt&quot;)&#10;        return self.format(&#10;            action=action,&#10;            current_resources_data=json.dumps(current_resources_data, indent=2)&#10;        )&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;    The model must return ONLY the Markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing ContentGenerationTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Style:&#10;{style}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and don't introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&#10;IMPORTANT: Output ONLY the markdown lesson content. Do NOT include any explanations, JSON, or extra text before or after the markdown.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting ContentGenerationTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing BlogGenerationPrompt Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting BlogGenerationPrompt for style: {style}&quot;)&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing RouteSelectorNode Template&quot;)&#10;        super().__init__(&#10;            template = '''&#10;You are a route selector for an educational learning system.&#10;Your task is to determine the next action based on the user''s current state and progress.&#10;Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resources: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting RouteSelectorNode prompt for action.&quot;)&#10;        return self.format(&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_FEEDBACK_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an intelligent feedback assistant trained to process and structure user feedback on educational content.&#10;&#10;Your goal is to analyze the provided content and comments, and generate a clean JSON object with the following fields ONLY:&#10;&#10;- `rating`: An integer from 1 to 5 (1 = very poor, 5 = excellent).&#10;- `comments`: A short criticised summary of the content.&#10;- `needed`: A boolean indicating if feedback is needed (True) or not (False).&#10;- `gaps`: A list of specific content gaps or areas for improvement (optional, can be empty).&#10;&#10;**Instructions:**&#10;- Return ONLY a valid JSON object with these four fields.&#10;- Do NOT include any extra text, explanation, or fields.&#10;- If a field is not available, still include it with a reasonable default (e.g., `comments` can be an empty string, `gaps` can be an empty list).&#10;- Never invent new fields or explanations.&#10;- Your output must be a single JSON object, nothing else.&#10;&#10;Example output:&#10;{&#10;  &quot;rating&quot;: 4,&#10;  &quot;comments&quot;: &quot;The explanation was clear and engaging, but could use more real-world examples.&quot;,&#10;  &quot;needed&quot;: true,&#10;  &quot;gaps&quot;: [&#10;    &quot;Missing real-world applications of the concept.&quot;,&#10;    &quot;Could include more visual aids.&quot;&#10;  ]&#10;}&#10;&quot;&quot;&quot;)&#10;&#10;class ContentGapGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to identify content gaps in educational material based on feedback and the content itself.&#10;    The LLM should return a JSON object matching the FeedBack class, with:&#10;      - rating: integer (1-5)&#10;      - comments: summary of feedback and identified gaps&#10;      - needed: boolean (True if improvement is needed)&#10;      - gaps: list of specific content gaps (optional, for next model)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        super().__init__(&#10;            template=(&#10;&quot;&quot;&quot;You are an expert educational content reviewer.&#10;&#10;Your task is to analyze the following learning content and the feedback provided, and identify any content gaps, missing explanations, unclear sections, or areas for improvement.&#10;&#10;CONTENT:&#10;{content}&#10;&#10;FEEDBACK:&#10;{feedback}&#10;&#10;Instructions:&#10;- Carefully read both the content and the feedback.&#10;- Identify and list all content gaps, missing details, or unclear explanations.&#10;- If the feedback already mentions gaps, include them. If you find additional gaps, add those too.&#10;- Return a JSON object with the following fields ONLY:&#10;  - &quot;rating&quot;: integer from 1 to 5 (overall quality)&#10;  - &quot;comments&quot;: a short summary of the main feedback and gaps&#10;  - &quot;needed&quot;: true if improvement is needed, false if not&#10;  - &quot;gaps&quot;: a list of specific content gaps or improvement points&#10;&#10;Example output:&#10;{{&#10;  &quot;rating&quot;: 3,&#10;  &quot;comments&quot;: &quot;The content is generally clear but lacks real-world examples and visual aids. Some sections are too brief.&quot;,&#10;  &quot;needed&quot;: true,&#10;  &quot;gaps&quot;: [&#10;    &quot;No real-world examples provided.&quot;,&#10;    &quot;Missing diagrams or visual explanations.&quot;,&#10;    &quot;The explanation of the formula derivation is too brief.&quot;&#10;  ]&#10;}}&#10;- Do NOT include any extra text or explanation outside the JSON.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;content&quot;, &quot;feedback&quot;]&#10;        )&#10;&#10;    def format_prompt(self, content: str, feedback: dict) -&gt; str:&#10;        return self.format(&#10;            content=content,&#10;            feedback=feedback&#10;        )&#10;&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_feedback = CONTENT_FEEDBACK_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;prompt_gap_finder = ContentGapGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichedLearningResource)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelector)&#10;content_generation = prompt_content_generation | get_gemini_model(ContentResponse)&#10;blog_generation = prompt_blog_generation | get_gemini_model(ContentResponse)&#10;gap_finder = prompt_gap_finder | get_gemini_model(FeedBack)&#10;content_improviser =get_groq_model()&#10;content_feedback = get_deepseek_model(FeedBack)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils.py" />
              <option name="updatedContent" value="import os&#10;import json&#10;import logging&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/utils.py" />
              <option name="originalContent" value="import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save generated content to {file_path}: {e}&quot;)&#10;" />
              <option name="updatedContent" value="import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Always use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save generated content to {file_path}: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>