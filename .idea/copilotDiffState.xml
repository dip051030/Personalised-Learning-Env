<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/db/vector_db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/vector_db.py" />
              <option name="originalContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Vector database utilities for building and saving ChromaDB collections from lesson and scraped data.&#10;&quot;&quot;&quot;&#10;import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Cleaned metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/external_tools_apis.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/external_tools_apis.py" />
              <option name="originalContent" value="import requests&#10;from langsmith import expect&#10;from sqlalchemy.testing.plugin.plugin_base import logging&#10;&#10;from keys.apis import set_env&#10;def serp_api_tool(query: str) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Use SerpAPI to search for the given query and return the results.&#10;    Returns a dictionary with the search results.&#10;    &quot;&quot;&quot;&#10;    data = {}&#10;&#10;    try:&#10;        api_key = set_env('SERP_API_KEY')&#10;        if not api_key:&#10;            raise ValueError(&quot;SERP_API_KEY is not set. Please set it in your environment variables.&quot;)&#10;&#10;        headers = {&#10;            'X-API-KEY': api_key,&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        params = {&#10;            'q': query + ' site:byjus.com OR site:vedantu.com OR site:toppr.com OR site:learnfatafat.com',&#10;            'engine': &quot;google&quot;,&#10;            'num': 10,&#10;            'gl': 'in'&#10;&#10;        }&#10;&#10;        response = requests.post('https://google.serper.dev/search', json=params, headers=headers)&#10;        data = response.json()&#10;&#10;    except Exception as e:&#10;        logging.error(f&quot;Error in SerpAPI tool: {e}&quot;)&#10;        data = {&quot;error&quot;: str(e)}&#10;&#10;    return data" />
              <option name="updatedContent" value="import requests&#10;import logging&#10;from keys.apis import set_env&#10;&#10;&#10;def serp_api_tool(query: str) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Use SerpAPI to search for the given query and return the results.&#10;    Returns a dictionary with the search results.&#10;    &quot;&quot;&quot;&#10;    data = {}&#10;&#10;    try:&#10;        api_key = set_env('SERP_API_KEY')&#10;        if not api_key:&#10;            raise ValueError(&quot;SERP_API_KEY is not set. Please set it in your environment variables.&quot;)&#10;&#10;        headers = {&#10;            'X-API-KEY': api_key,&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        params = {&#10;            'q': query + ' site:byjus.com OR site:vedantu.com OR site:toppr.com OR site:learnfatafat.com',&#10;            'engine': &quot;google&quot;,&#10;            'num': 10,&#10;            'gl': 'in'&#10;&#10;        }&#10;&#10;        response = requests.post('https://google.serper.dev/search', json=params, headers=headers)&#10;        data = response.json()&#10;&#10;    except Exception as e:&#10;        logging.error(f&quot;Error in SerpAPI tool: {e}&quot;)&#10;        data = {&quot;error&quot;: str(e)}&#10;&#10;    return data" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scrapper/crawl4ai_scrapping.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scrapper/crawl4ai_scrapping.py" />
              <option name="originalContent" value="import logging&#10;from crawl4ai import (&#10;    AsyncWebCrawler,&#10;    BrowserConfig,&#10;    CrawlerRunConfig,&#10;    LLMExtractionStrategy,&#10;    LLMConfig, CacheMode&#10;)&#10;from datetime import datetime&#10;import json&#10;from keys.apis import set_env&#10;from schemas import WebCrawlerConfig&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;async def crawl_and_extract_json(urls: list) -&gt; list:&#10;&#10;    browser_cfg = BrowserConfig(&#10;        browser_type=&quot;firefox&quot;,&#10;        headless=False,&#10;        verbose=True,&#10;        light_mode=False&#10;    )&#10;&#10;&#10;&#10;    api_token = set_env('GROQ_DEEPSEEK_API_KEY')&#10;    if not api_token:&#10;        raise ValueError(&quot;Environment variable 'GROQ_DEEPSEEK_API_KEY' is not set or invalid.&quot;)&#10;&#10;    llm_cfg = LLMConfig(&#10;        provider='ollama/llama3',&#10;        temperature=0,&#10;    )&#10;&#10;    extraction_strategy = LLMExtractionStrategy(&#10;        llm_config=llm_cfg,&#10;        instruction=&quot;&quot;&quot;&#10;        You are an intelligent educational content extractor. You must output a clean, valid JSON object **exactly** following the schema and rules below. If data is not present on the page, return empty lists or nulls — but you must still return a full valid JSON object.&#10;&#10;        DO NOT skip the output. DO NOT return natural language. DO NOT explain anything.&#10;&#10;        ---&#10;&#10;        STRICT JSON Output Schema:&#10;        {&#10;          &quot;url&quot;: &quot;&lt;original URL&gt;&quot;,  // always required&#10;          &quot;title&quot;: &quot;&lt;main article H1 title or null&gt;&quot;,  &#10;          &quot;headings&quot;: [&quot;&lt;H2–H4 section/sub-section headings&gt;&quot;],  &#10;          &quot;main_findings&quot;: [&quot;&lt;key educational facts, definitions, or concepts in full sentences&gt;&quot;],  &#10;          &quot;content&quot;: &quot;&lt;single combined block of all main findings in original order or null if none&gt;&quot;,  &#10;          &quot;keywords&quot;: [&quot;&lt;list of core nouns or terms from headings and findings or []&gt;&quot;]&#10;        }&#10;&#10;        ---&#10;&#10;        Rules:&#10;&#10;        Must Include:&#10;        - Main article title (`&lt;h1&gt;` tag or first major visible title)&#10;        - All `&lt;h2&gt;`, `&lt;h3&gt;`, and `&lt;h4&gt;` tags from the **main content body only**&#10;        - Individual full-sentence educational statements — laws, definitions, examples, formulas, etc.&#10;        - Bullet points **only if** each is a full sentence with factual educational info&#10;        - Join all findings in `content` (preserve exact sentence order and original phrasing)&#10;        - Extract `keywords` by identifying repeated or important terms in `headings` + `main_findings`&#10;&#10;        Must Exclude:&#10;        - Website name, branding, menus, footers, navbars, cookie banners, popups, forms, ads&#10;        - Code blocks, timestamps, author bios, comments, vague filler, layout-only text&#10;        - External links, social media, UI elements, inline styles, HTML/CSS/JS&#10;&#10;        ---&#10;&#10;        Output Guidelines:&#10;        - If any field is missing from the page, return:&#10;          - `null` for title or content&#10;          - `[]` for headings, main_findings, or keywords&#10;        - The `&quot;url&quot;` field is always required and must match the input&#10;        - Output must be **strictly valid JSON** — no markdown, no commentary, no extra text&#10;&#10;        ---&#10;&#10;        YOUR ROLE:&#10;        You are a **non-generative extractor** — not a writer. Never summarize, paraphrase, or invent.&#10;        Your goal is to extract clean, structured data for curriculum systems and AI tutors.&#10;&#10;        ALWAYS return full JSON — even if data is minimal.&#10;        &quot;&quot;&quot;&#10;        ,&#10;    input_format='markdown',&#10;        schema=WebCrawlerConfig.model_json_schema()&#10;    )&#10;&#10;    crawl_cfg = CrawlerRunConfig(&#10;        magic=True,&#10;        excluded_tags=[&#10;            'footer',&#10;            'nav',&#10;            'aside',&#10;            'script',&#10;            'style',&#10;            'link',&#10;            'form',&#10;            'noscript',&#10;            'iframe',&#10;            'svg',&#10;            'canvas',&#10;            'input',&#10;            'button',&#10;            'select',&#10;            'option',&#10;            'label',&#10;            'object',&#10;            'embed',&#10;            'video',&#10;            'audio'&#10;        ],&#10;        excluded_selector=&#10;            '.ads, .advertisement, .sponsored, .promo, .sidebar, .related-links, .comments, .comment, '&#10;            '.social-links, .share-buttons, .social-media, .footer, .footer-links, .footer-info, '&#10;            '.footer-text, .footer-logo, .footer-social, .footer-contact, .footer-legal, .footer-privacy, '&#10;            '.footer-terms, .footer-copyright, .footer-disclaimer, .footer-sitemap, '&#10;            '.footer-subscribe, .footer-newsletter, .footer-contact-form, .footer-address, '&#10;            '.footer-menu, .cookie-notice, .cookie-banner, .cookies, .popup, .modal, '&#10;            '.popup-overlay, .popup-content, .popup-close, .popup-header, .popup-body, '&#10;            '.popup-footer, .popup-buttons, .popup-link, .login, .signup, .login-form, '&#10;            '.register, .auth, .nav, .navbar, .navigation, .menu, .topbar, .toolbar, '&#10;            '.header, .masthead, .banner, .cta, .newsletter, .subscribe, .sticky, '&#10;            '.chatbot, .livechat, .intercom-launcher, .notifications, .alert, .announcement, '&#10;            '.breadcrumb, .pagination, .loader, .loading, .spinner, .hero, .widget, .widget-area, '&#10;            '.search-box, .search-form, .search-bar, .scroll-to-top, .back-to-top, .branding, '&#10;            '.related-posts, .related-articles, .more-articles, .external-links, .print-button',&#10;        only_text = True,&#10;        remove_forms=True,&#10;        exclude_external_links=True,&#10;        exclude_social_media_links=True,&#10;        verbose=True,&#10;        extraction_strategy=extraction_strategy,&#10;    )&#10;&#10;    results = []&#10;    async with AsyncWebCrawler(config=browser_cfg) as crawler:&#10;        for url in urls:&#10;            try:&#10;                logging.info(f&quot;Crawling -&gt; {url}&quot;)&#10;&#10;                result = await crawler.arun(&#10;                    url=url,&#10;                    config=crawl_cfg&#10;                )&#10;&#10;                if result.success:&#10;                    logging.info(f&quot;Successfully crawled {url}&quot;)&#10;&#10;                    extracted_list = json.loads(result.extracted_content)&#10;&#10;                    extracted = extracted_list[0] if isinstance(extracted_list, list) else extracted_list&#10;&#10;                    results.append({&#10;                        &quot;url&quot;: extracted.get(&quot;url&quot;, url),&#10;                        &quot;source&quot;: extracted.get(&quot;source&quot;, &quot;&quot;),  # e.g. &quot;byjus.com&quot;&#10;                        &quot;subject&quot;: extracted.get(&quot;subject&quot;, &quot;&quot;),  # e.g. &quot;Physics&quot;&#10;                        &quot;grade&quot;: extracted.get(&quot;grade&quot;, None),  # e.g. 11 (int) or None&#10;                        &quot;unit&quot;: extracted.get(&quot;unit&quot;, &quot;&quot;),  # e.g. &quot;Electricity and Magnetism&quot;&#10;                        &quot;topic_title&quot;: extracted.get(&quot;topic_title&quot;, None),  # optional, e.g. &quot;Coulomb’s law&quot;&#10;                        &quot;title&quot;: extracted.get(&quot;title&quot;),&#10;                        &quot;headings&quot;: extracted.get(&quot;headings&quot;, []),&#10;                        &quot;main_findings&quot;: extracted.get(&quot;main_findings&quot;, []),&#10;                        &quot;content&quot;: extracted.get(&quot;content&quot;),&#10;                        &quot;keywords&quot;: extracted.get(&quot;keywords&quot;, []),  # new field for keywords&#10;                        &quot;word_count&quot;: extracted.get(&#10;                            &quot;word_count&quot;,&#10;                            len(&quot; &quot;.join(extracted.get(&quot;main_findings&quot;, [])).split())&#10;                        ),&#10;                        &quot;status&quot;: &quot;success&quot;,&#10;                        &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                    })&#10;                    extraction_strategy.show_usage()&#10;&#10;            except Exception as e:&#10;                logging.error(f&quot;Error crawling {url}: {e}&quot;)&#10;                results.append({&#10;                    &quot;url&quot;: url,&#10;                    &quot;source&quot;: &quot;&quot;,&#10;                    &quot;subject&quot;: &quot;&quot;,&#10;                    &quot;grade&quot;: None,&#10;                    &quot;unit&quot;: &quot;&quot;,&#10;                    &quot;topic_title&quot;: None,&#10;                    &quot;title&quot;: None,&#10;                    &quot;headings&quot;: [],&#10;                    &quot;main_findings&quot;: [],&#10;                    &quot;content&quot;: None,&#10;                    &quot;keywords&quot;: [],&#10;                    &quot;word_count&quot;: 0,&#10;                    &quot;status&quot;: &quot;failed&quot;,&#10;                    &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                })&#10;&#10;    return results" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Educational content crawling and extraction utilities using crawl4ai and LLM strategies.&#10;&quot;&quot;&quot;&#10;import logging&#10;from crawl4ai import (&#10;    AsyncWebCrawler,&#10;    BrowserConfig,&#10;    CrawlerRunConfig,&#10;    LLMExtractionStrategy,&#10;    LLMConfig, CacheMode&#10;)&#10;from datetime import datetime&#10;import json&#10;from keys.apis import set_env&#10;from schemas import WebCrawlerConfig&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;async def crawl_and_extract_json(urls: list) -&gt; list:&#10;    &quot;&quot;&quot;&#10;    Crawl a list of URLs and extract educational content as JSON objects.&#10;    Args:&#10;        urls (list): List of URLs to crawl.&#10;    Returns:&#10;        list: List of extracted JSON objects for each URL.&#10;    &quot;&quot;&quot;&#10;    browser_cfg = BrowserConfig(&#10;        browser_type=&quot;firefox&quot;,&#10;        headless=False,&#10;        verbose=True,&#10;        light_mode=False&#10;    )&#10;&#10;    api_token = set_env('GROQ_DEEPSEEK_API_KEY')&#10;    if not api_token:&#10;        raise ValueError(&quot;Environment variable 'GROQ_DEEPSEEK_API_KEY' is not set or invalid.&quot;)&#10;&#10;    llm_cfg = LLMConfig(&#10;        provider='ollama/llama3',&#10;        temperature=0,&#10;    )&#10;&#10;    extraction_strategy = LLMExtractionStrategy(&#10;        llm_config=llm_cfg,&#10;        instruction=&quot;&quot;&quot;&#10;        You are an intelligent educational content extractor. You must output a clean, valid JSON object **exactly** following the schema and rules below. If data is not present on the page, return empty lists or nulls — but you must still return a full valid JSON object.&#10;&#10;        DO NOT skip the output. DO NOT return natural language. DO NOT explain anything.&#10;&#10;        ---&#10;&#10;        STRICT JSON Output Schema:&#10;        {&#10;          &quot;url&quot;: &quot;&lt;original URL&gt;&quot;,  // always required&#10;          &quot;title&quot;: &quot;&lt;main article H1 title or null&gt;&quot;,  &#10;          &quot;headings&quot;: [&quot;&lt;H2–H4 section/sub-section headings&gt;&quot;],  &#10;          &quot;main_findings&quot;: [&quot;&lt;key educational facts, definitions, or concepts in full sentences&gt;&quot;],  &#10;          &quot;content&quot;: &quot;&lt;single combined block of all main findings in original order or null if none&gt;&quot;,  &#10;          &quot;keywords&quot;: [&quot;&lt;list of core nouns or terms from headings and findings or []&gt;&quot;]&#10;        }&#10;&#10;        ---&#10;&#10;        Rules:&#10;&#10;        Must Include:&#10;        - Main article title (`&lt;h1&gt;` tag or first major visible title)&#10;        - All `&lt;h2&gt;`, `&lt;h3&gt;`, and `&lt;h4&gt;` tags from the **main content body only**&#10;        - Individual full-sentence educational statements — laws, definitions, examples, formulas, etc.&#10;        - Bullet points **only if** each is a full sentence with factual educational info&#10;        - Join all findings in `content` (preserve exact sentence order and original phrasing)&#10;        - Extract `keywords` by identifying repeated or important terms in `headings` + `main_findings`&#10;&#10;        Must Exclude:&#10;        - Website name, branding, menus, footers, navbars, cookie banners, popups, forms, ads&#10;        - Code blocks, timestamps, author bios, comments, vague filler, layout-only text&#10;        - External links, social media, UI elements, inline styles, HTML/CSS/JS&#10;&#10;        ---&#10;&#10;        Output Guidelines:&#10;        - If any field is missing from the page, return:&#10;          - `null` for title or content&#10;          - `[]` for headings, main_findings, or keywords&#10;        - The `&quot;url&quot;` field is always required and must match the input&#10;        - Output must be **strictly valid JSON** — no markdown, no commentary, no extra text&#10;&#10;        ---&#10;&#10;        YOUR ROLE:&#10;        You are a **non-generative extractor** — not a writer. Never summarize, paraphrase, or invent.&#10;        Your goal is to extract clean, structured data for curriculum systems and AI tutors.&#10;&#10;        ALWAYS return full JSON — even if data is minimal.&#10;        &quot;&quot;&quot;&#10;        ,&#10;    input_format='markdown',&#10;        schema=WebCrawlerConfig.model_json_schema()&#10;    )&#10;&#10;    crawl_cfg = CrawlerRunConfig(&#10;        magic=True,&#10;        excluded_tags=[&#10;            'footer',&#10;            'nav',&#10;            'aside',&#10;            'script',&#10;            'style',&#10;            'link',&#10;            'form',&#10;            'noscript',&#10;            'iframe',&#10;            'svg',&#10;            'canvas',&#10;            'input',&#10;            'button',&#10;            'select',&#10;            'option',&#10;            'label',&#10;            'object',&#10;            'embed',&#10;            'video',&#10;            'audio'&#10;        ],&#10;        excluded_selector=&#10;            '.ads, .advertisement, .sponsored, .promo, .sidebar, .related-links, .comments, .comment, '&#10;            '.social-links, .share-buttons, .social-media, .footer, .footer-links, .footer-info, '&#10;            '.footer-text, .footer-logo, .footer-social, .footer-contact, .footer-legal, .footer-privacy, '&#10;            '.footer-terms, .footer-copyright, .footer-disclaimer, .footer-sitemap, '&#10;            '.footer-subscribe, .footer-newsletter, .footer-contact-form, .footer-address, '&#10;            '.footer-menu, .cookie-notice, .cookie-banner, .cookies, .popup, .modal, '&#10;            '.popup-overlay, .popup-content, .popup-close, .popup-header, .popup-body, '&#10;            '.popup-footer, .popup-buttons, .popup-link, .login, .signup, .login-form, '&#10;            '.register, .auth, .nav, .navbar, .navigation, .menu, .topbar, .toolbar, '&#10;            '.header, .masthead, .banner, .cta, .newsletter, .subscribe, .sticky, '&#10;            '.chatbot, .livechat, .intercom-launcher, .notifications, .alert, .announcement, '&#10;            '.breadcrumb, .pagination, .loader, .loading, .spinner, .hero, .widget, .widget-area, '&#10;            '.search-box, .search-form, .search-bar, .scroll-to-top, .back-to-top, .branding, '&#10;            '.related-posts, .related-articles, .more-articles, .external-links, .print-button',&#10;        only_text = True,&#10;        remove_forms=True,&#10;        exclude_external_links=True,&#10;        exclude_social_media_links=True,&#10;        verbose=True,&#10;        extraction_strategy=extraction_strategy,&#10;    )&#10;&#10;    results = []&#10;    async with AsyncWebCrawler(config=browser_cfg) as crawler:&#10;        for url in urls:&#10;            try:&#10;                logging.info(f&quot;Crawling -&gt; {url}&quot;)&#10;&#10;                result = await crawler.arun(&#10;                    url=url,&#10;                    config=crawl_cfg&#10;                )&#10;&#10;                if result.success:&#10;                    logging.info(f&quot;Successfully crawled {url}&quot;)&#10;&#10;                    extracted_list = json.loads(result.extracted_content)&#10;&#10;                    extracted = extracted_list[0] if isinstance(extracted_list, list) else extracted_list&#10;&#10;                    results.append({&#10;                        &quot;url&quot;: extracted.get(&quot;url&quot;, url),&#10;                        &quot;source&quot;: extracted.get(&quot;source&quot;, &quot;&quot;),  # e.g. &quot;byjus.com&quot;&#10;                        &quot;subject&quot;: extracted.get(&quot;subject&quot;, &quot;&quot;),  # e.g. &quot;Physics&quot;&#10;                        &quot;grade&quot;: extracted.get(&quot;grade&quot;, None),  # e.g. 11 (int) or None&#10;                        &quot;unit&quot;: extracted.get(&quot;unit&quot;, &quot;&quot;),  # e.g. &quot;Electricity and Magnetism&quot;&#10;                        &quot;topic_title&quot;: extracted.get(&quot;topic_title&quot;, None),  # optional, e.g. &quot;Coulomb’s law&quot;&#10;                        &quot;title&quot;: extracted.get(&quot;title&quot;),&#10;                        &quot;headings&quot;: extracted.get(&quot;headings&quot;, []),&#10;                        &quot;main_findings&quot;: extracted.get(&quot;main_findings&quot;, []),&#10;                        &quot;content&quot;: extracted.get(&quot;content&quot;),&#10;                        &quot;keywords&quot;: extracted.get(&quot;keywords&quot;, []),  # new field for keywords&#10;                        &quot;word_count&quot;: extracted.get(&#10;                            &quot;word_count&quot;,&#10;                            len(&quot; &quot;.join(extracted.get(&quot;main_findings&quot;, [])).split())&#10;                        ),&#10;                        &quot;status&quot;: &quot;success&quot;,&#10;                        &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                    })&#10;                    extraction_strategy.show_usage()&#10;&#10;            except Exception as e:&#10;                logging.error(f&quot;Error crawling {url}: {e}&quot;)&#10;                results.append({&#10;                    &quot;url&quot;: url,&#10;                    &quot;source&quot;: &quot;&quot;,&#10;                    &quot;subject&quot;: &quot;&quot;,&#10;                    &quot;grade&quot;: None,&#10;                    &quot;unit&quot;: &quot;&quot;,&#10;                    &quot;topic_title&quot;: None,&#10;                    &quot;title&quot;: None,&#10;                    &quot;headings&quot;: [],&#10;                    &quot;main_findings&quot;: [],&#10;                    &quot;content&quot;: None,&#10;                    &quot;keywords&quot;: [],&#10;                    &quot;word_count&quot;: 0,&#10;                    &quot;status&quot;: &quot;failed&quot;,&#10;                    &quot;scraped_at&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;&#10;                })&#10;&#10;    return results" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils.py" />
              <option name="updatedContent" value="import os&#10;import json&#10;import logging&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/utils.py" />
              <option name="originalContent" value="import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Always use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save generated content to {file_path}: {e}&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Utility functions for saving learning state and generated content to files.&#10;&quot;&quot;&quot;&#10;import os&#10;import json&#10;import logging&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object or dict to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method) or dict&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Always use model_dump(mode=&quot;json&quot;) for full serialization of nested models&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump(mode=&quot;json&quot;)&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        elif isinstance(state, dict):&#10;            state_data = state&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;&#10;def save_generated_content(content, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the generated content (string) to a separate file.&#10;    If the file's directory does not exist, it will be created.&#10;    Args:&#10;        content: The generated content as a string.&#10;        file_path: Path to the file where content will be saved.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            if not os.path.exists(dir_name):&#10;                os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            f.write(content)&#10;        logging.info(f&quot;Generated content saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save generated content to {file_path}: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>