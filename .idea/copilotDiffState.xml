<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/db/loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/loader.py" />
              <option name="originalContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot;&#10;&#10;&#10;def load_json_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;&#10;    Load lesson data from a JSON file in the lessons data directory.&#10;&#10;    Args:&#10;        filename (str): The name of the lesson data file.&#10;&#10;    Returns:&#10;        List[Dict[str, Any]]: List of lesson data dictionaries.&#10;&#10;    Raises:&#10;        FileNotFoundError: If the file does not exist in the data directory.&#10;    &quot;&quot;&quot;&#10;    path = DATA_DIR / filename&#10;    logging.info(f&quot;Loading lesson data from {path}&quot;)&#10;    if not path.exists():&#10;        logging.error(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;    logging.info(f&quot;Loaded {len(data)} lessons from {filename}&quot;)&#10;    return data&#10;" />
              <option name="updatedContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot;&#10;&#10;&#10;def load_json_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;&#10;    Load lesson data from a JSON file in the lessons data directory.&#10;&#10;    Args:&#10;        filename (str): The name of the lesson data file.&#10;&#10;    Returns:&#10;        List[Dict[str, Any]]: List of lesson data dictionaries.&#10;&#10;    Raises:&#10;        FileNotFoundError: If the file does not exist in the data directory.&#10;    &quot;&quot;&quot;&#10;    path = DATA_DIR / filename&#10;    try:&#10;        logging.info(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] INFO Loading lesson data from {path}&quot;)&#10;        if not path.exists():&#10;            logging.error(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] ERROR {filename} not found in {DATA_DIR}&quot;)&#10;            raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;            data = json.load(f)&#10;        logging.info(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] INFO Loaded {len(data)} lessons from {filename}&quot;)&#10;        return data&#10;    except Exception as e:&#10;        logging.error(f&quot;[loader.py:{load_json_data.__code__.co_firstlineno}] ERROR Failed to load lesson data: {e}&quot;)&#10;        return []" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/db/vector_db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/vector_db.py" />
              <option name="originalContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Vector database utilities for building and saving ChromaDB collections from lesson and scraped data.&#10;&quot;&quot;&quot;&#10;import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_json_data&#10;import logging&#10;import json&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def sanitize_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Convert list values in metadata to comma-separated strings for ChromaDB compatibility.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Sanitized metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: (&quot;,&quot;.join(v) if isinstance(v, list) else v) for k, v in metadata.items()}&#10;&#10;&#10;def clean_metadata(metadata: dict) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Remove keys with None values from metadata.&#10;&#10;    Args:&#10;        metadata (dict): Metadata dictionary.&#10;&#10;    Returns:&#10;        dict: Cleaned metadata dictionary.&#10;    &quot;&quot;&quot;&#10;    return {k: v for k, v in metadata.items() if v is not None}&#10;&#10;def build_chroma_db_collection(filename: str = 'lessons/class_12_physics.json', collection_name: str = 'lessons'):&#10;    &quot;&quot;&quot;&#10;    Build a ChromaDB collection from lesson data and return the collection and embedding model.&#10;&#10;    Args:&#10;        filename (str): The lesson data filename.&#10;        collection_name (str): The name for the ChromaDB collection.&#10;&#10;    Returns:&#10;        tuple: (collection, embedding model)&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_json_data(filename)&#10;    model = SentenceTransformer('Shashwat13333/bge-base-en-v1.5_v4')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')} {lesson.get('elaboration', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings FROM LOCAL DB!&quot;)&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;),&#10;            'description': lesson.get('description', ''),&#10;            'elaboration': lesson.get('elaboration', '')&#10;        }&#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path = './local VDB/chromadb')&#10;    logging.info(&quot;Connecting to ChromaDB&quot;)&#10;    collection = client.get_or_create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas= [sanitize_metadata(metadata) for metadata in metadatas]&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;&#10;&#10;def save_scraped_data_to_vdb(&#10;    scraped_file: str = &quot;raw_data.json&quot;,&#10;    vdb_path: str = &quot;./local VDB/chromadb&quot;,&#10;    collection_name: str = &quot;scraped_data&quot;&#10;):&#10;    &quot;&quot;&quot;&#10;    Save scraped data from a JSON file to ChromaDB vector database.&#10;&#10;    Args:&#10;        scraped_file (str): Path to the scraped data JSON file.&#10;        vdb_path (str): Path to the ChromaDB persistent directory.&#10;        collection_name (str): Name of the ChromaDB collection.&#10;    &quot;&quot;&quot;&#10;    logging.info(f&quot;Loading scraped data from {scraped_file}&quot;)&#10;    scrapped_data = load_json_data(scraped_file)&#10;    scrapped_documents = [f'{item.get('main_findings')} {item.get('keywords')} {item.get('headings')}' for item in scrapped_data]&#10;&#10;    logging.info(f&quot;Encoding {len(scrapped_documents)} documents for embeddings&quot;)&#10;    model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;    embeddings = model.encode(scrapped_documents, show_progress_bar=True).tolist()&#10;    logging.info(f&quot;Encoded {len(embeddings)} embeddings OF SCRAPPED DATA!&quot;)&#10;    scrapped_meta = [&#10;        {&#10;            &quot;headings&quot;: item.get(&quot;headings&quot;, []),&#10;            &quot;main_findings&quot;: item.get(&quot;main_findings&quot;, []),&#10;            &quot;keywords&quot;: item.get(&quot;keywords&quot;, []),&#10;        }&#10;        for item in scrapped_data&#10;    ]&#10;&#10;    client = chromadb.PersistentClient(path=vdb_path)&#10;    collection = client.get_or_create_collection(collection_name)&#10;&#10;    collection.add(&#10;        ids= [str(i) for i in range(1, len(scrapped_data) + 1)],&#10;        embeddings= embeddings,&#10;        documents=scrapped_documents,&#10;        metadatas=[sanitize_metadata(_) for _ in scrapped_meta]&#10;        )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/logis/logical_functions.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/logis/logical_functions.py" />
              <option name="originalContent" value="import logging&#10;import chromadb&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack, ContentType&#10;from db.vector_db import build_chroma_db_collection, save_scraped_data_to_vdb&#10;from sentence_transformers import SentenceTransformer&#10;&#10;&#10;def search_both_collections(state : LearningState,&#10;                            vdb_path=&quot;./local VDB/chromadb&quot;,&#10;                            lessons_collection=&quot;lessons&quot;,&#10;                            scraped_collection=&quot;scraped_data&quot;,&#10;                            n_results=1):&#10;    &quot;&quot;&quot;&#10;    Search both the lessons and scraped_data collections for the most similar items to the query.&#10;    Returns results from both collections.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is None:&#10;            return None&#10;&#10;&#10;        build_chroma_db_collection()&#10;        save_scraped_data_to_vdb()&#10;        # Load the embedding model&#10;        model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;        query_text = state.current_resource.topic&#10;        query_embedding = model.encode(query_text).tolist()&#10;&#10;        # Connect to ChromaDB&#10;        client = chromadb.PersistentClient(path=vdb_path)&#10;        lessons_col = client.get_or_create_collection(lessons_collection)&#10;        scraped_col = client.get_or_create_collection(scraped_collection)&#10;&#10;        # Query both collections&#10;        lessons_results = lessons_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;        scraped_results = scraped_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;&#10;        return {&#10;            &quot;lessons_results&quot;: lessons_results,&#10;            &quot;scraped_results&quot;: scraped_results&#10;        }&#10;    except Exception as e:&#10;        logging.error(f&quot;Error searching collections: {e}&quot;)&#10;        return None&#10;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on curriculum metadata, topic type, and phrasing.&#10;    Returns a string indicating the lesson style.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic.lower()&#10;    unit = state.current_resource.unit.lower()&#10;    desc = state.current_resource.description.lower()&#10;&#10;    if &quot;evaluation&quot; in unit:&#10;        style = &quot;evaluation_component&quot;&#10;    elif &quot;practical&quot; in state.current_resource.topic_id or &quot;activity&quot; in topic or &quot;experiment&quot; in desc:&#10;        style = &quot;experimental&quot;&#10;    elif any(keyword in topic for keyword in [&quot;derive&quot;, &quot;calculate&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;formula&quot;]):&#10;        style = &quot;problem_solving&quot;&#10;    elif any(keyword in desc for keyword in [&quot;used in&quot;, &quot;applied in&quot;, &quot;application&quot;, &quot;real-world&quot;]):&#10;        style = &quot;application_based&quot;&#10;    elif &quot;revision&quot; in topic or &quot;summary&quot; in topic:&#10;        style = &quot;revision_summary&quot;&#10;    elif &quot;quiz&quot; in topic or state.content_type == ContentType.QUIZ:&#10;        style = &quot;interactive_quiz&quot;&#10;    elif &quot;enrich&quot; in topic or &quot;context&quot; in desc:&#10;        style = &quot;enrichment&quot;&#10;    else:&#10;        style = &quot;conceptual_focus&quot;&#10;&#10;    return style&#10;&#10;&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    Returns a LearningResource instance.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata.get('subject', 'unknown').lower()),&#10;        grade=metadata.get(&quot;grade&quot;),&#10;        unit=metadata.get(&quot;unit&quot;),&#10;        topic_id=metadata.get(&quot;topic_id&quot;),&#10;        topic=metadata.get(&quot;topic_title&quot;),&#10;        description=metadata.get(&quot;description&quot;, &quot;&quot;),&#10;        keywords=metadata.get(&quot;keywords&quot;).split(&quot;,&quot;),&#10;        hours=metadata.get(&quot;hours&quot;),&#10;        references=metadata.get(&quot;references&quot;),&#10;        elaboration=metadata.get(&quot;elaboration&quot;, &quot;&quot;)&#10;    )&#10;&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    Returns a string indicating the blog style.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style&#10;&#10;def update_content_count(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Check the content count in the learning state.&#10;    Returns a string indicating if an update is required.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.count &lt; 4:&#10;            logging.info(f&quot;Current state count: {state.count}&quot;)&#10;            return 'Update required'&#10;        else:&#10;            return 'No update required'&#10;    except Exception as e:&#10;        logging.error(f&quot;Error updating state count: {e}&quot;)&#10;        return 'No update required'" />
              <option name="updatedContent" value="import logging&#10;import chromadb&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack, ContentType&#10;from db.vector_db import build_chroma_db_collection, save_scraped_data_to_vdb&#10;from sentence_transformers import SentenceTransformer&#10;&#10;&#10;def search_both_collections(state : LearningState,&#10;                            vdb_path=&quot;./local VDB/chromadb&quot;,&#10;                            lessons_collection=&quot;lessons&quot;,&#10;                            scraped_collection=&quot;scraped_data&quot;,&#10;                            n_results=1):&#10;    &quot;&quot;&quot;&#10;    Search both the lessons and scraped_data collections for the most similar items to the query.&#10;    Returns results from both collections.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is None:&#10;            logging.warning(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] WARNING No current_resource in state.&quot;)&#10;            return None&#10;&#10;&#10;        build_chroma_db_collection()&#10;        save_scraped_data_to_vdb()&#10;        # Load the embedding model&#10;        model = SentenceTransformer(&quot;Shashwat13333/bge-base-en-v1.5_v4&quot;)&#10;        query_text = state.current_resource.topic&#10;        query_embedding = model.encode(query_text).tolist()&#10;&#10;        # Connect to ChromaDB&#10;        client = chromadb.PersistentClient(path=vdb_path)&#10;        lessons_col = client.get_or_create_collection(lessons_collection)&#10;        scraped_col = client.get_or_create_collection(scraped_collection)&#10;&#10;        # Query both collections&#10;        lessons_results = lessons_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;        scraped_results = scraped_col.query(&#10;            query_embeddings=query_embedding,&#10;            n_results=n_results&#10;        )&#10;&#10;        logging.info(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] INFO Queried both collections for topic '{query_text}'.&quot;)&#10;        return {&#10;            &quot;lessons_results&quot;: lessons_results,&#10;            &quot;scraped_results&quot;: scraped_results&#10;        }&#10;    except Exception as e:&#10;        logging.error(f&quot;[logical_functions.py:{search_both_collections.__code__.co_firstlineno}] ERROR Error searching collections: {e}&quot;)&#10;        return None&#10;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on curriculum metadata, topic type, and phrasing.&#10;    Returns a string indicating the lesson style.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic.lower()&#10;    unit = state.current_resource.unit.lower()&#10;    desc = state.current_resource.description.lower()&#10;&#10;    if &quot;evaluation&quot; in unit:&#10;        style = &quot;evaluation_component&quot;&#10;    elif &quot;practical&quot; in state.current_resource.topic_id or &quot;activity&quot; in topic or &quot;experiment&quot; in desc:&#10;        style = &quot;experimental&quot;&#10;    elif any(keyword in topic for keyword in [&quot;derive&quot;, &quot;calculate&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;formula&quot;]):&#10;        style = &quot;problem_solving&quot;&#10;    elif any(keyword in desc for keyword in [&quot;used in&quot;, &quot;applied in&quot;, &quot;application&quot;, &quot;real-world&quot;]):&#10;        style = &quot;application_based&quot;&#10;    elif &quot;revision&quot; in topic or &quot;summary&quot; in topic:&#10;        style = &quot;revision_summary&quot;&#10;    elif &quot;quiz&quot; in topic or state.content_type == ContentType.QUIZ:&#10;        style = &quot;interactive_quiz&quot;&#10;    elif &quot;enrich&quot; in topic or &quot;context&quot; in desc:&#10;        style = &quot;enrichment&quot;&#10;    else:&#10;        style = &quot;conceptual_focus&quot;&#10;&#10;    return style&#10;&#10;&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    Returns a LearningResource instance.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata.get('subject', 'unknown').lower()),&#10;        grade=metadata.get(&quot;grade&quot;),&#10;        unit=metadata.get(&quot;unit&quot;),&#10;        topic_id=metadata.get(&quot;topic_id&quot;),&#10;        topic=metadata.get(&quot;topic_title&quot;),&#10;        description=metadata.get(&quot;description&quot;, &quot;&quot;),&#10;        keywords=metadata.get(&quot;keywords&quot;).split(&quot;,&quot;),&#10;        hours=metadata.get(&quot;hours&quot;),&#10;        references=metadata.get(&quot;references&quot;),&#10;        elaboration=metadata.get(&quot;elaboration&quot;, &quot;&quot;)&#10;    )&#10;&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    Returns a string indicating the blog style.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style&#10;&#10;def update_content_count(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Check the content count in the learning state.&#10;    Returns a string indicating if an update is required.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.count &lt; 4:&#10;            logging.info(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] INFO Current state count: {state.count}&quot;)&#10;            return 'Update required'&#10;        else:&#10;            logging.info(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] INFO No update required, current count: {state.count}&quot;)&#10;            return 'No update required'&#10;    except Exception as e:&#10;        logging.error(f&quot;[logical_functions.py:{update_content_count.__code__.co_firstlineno}] ERROR Error updating state count: {e}&quot;)&#10;        return 'No update required'" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scrapper/save_to_local.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scrapper/save_to_local.py" />
              <option name="originalContent" value="import json&#10;from typing import Union&#10;&#10;from models.external_tools_apis import serp_api_tool&#10;from schemas import LearningState&#10;&#10;def serper_api_results_parser(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Parses the results from the SerpAPI tool based on the current learning state.&#10;&#10;    Args:&#10;        state (LearningState): The current learning state containing the topic and grade.&#10;&#10;    Returns:&#10;        dict: The search results retrieved from the SerpAPI tool.&#10;    &quot;&quot;&quot;&#10;    serpapi_search_results = serp_api_tool(&#10;        query=state.current_resource.topic + 'for grade ' + str(state.current_resource.grade))&#10;    return serpapi_search_results&#10;&#10;&#10;def save_to_local(data: Union[dict, list], file_path: str):&#10;    &quot;&quot;&quot;&#10;    Saves the provided data (dict or list) to a local JSON file.&#10;&#10;    Args:&#10;        data (dict or list): The data to be saved.&#10;        file_path (str): The path where the data will be saved.&#10;&#10;    Raises:&#10;        TypeError: If the data contains unsupported types for JSON serialization.&#10;    &quot;&quot;&quot;&#10;    if not isinstance(data, (dict, list)):&#10;        raise TypeError(&quot;Data must be a dict or a list to be saved as JSON.&quot;)&#10;    with open(file_path, mode='w', encoding='utf-8') as f:&#10;        json.dump(data, f, indent=4, ensure_ascii=False)" />
              <option name="updatedContent" value="import json&#10;import logging&#10;import os&#10;from typing import Union&#10;&#10;from models.external_tools_apis import serp_api_tool&#10;from schemas import LearningState&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def serper_api_results_parser(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Parses the results from the SerpAPI tool based on the current learning state.&#10;&#10;    Args:&#10;        state (LearningState): The current learning state containing the topic and grade.&#10;&#10;    Returns:&#10;        dict: The search results retrieved from the SerpAPI tool.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        serpapi_search_results = serp_api_tool(&#10;            query=state.current_resource.topic + ' for grade ' + str(state.current_resource.grade))&#10;        logging.info(f&quot;[save_to_local.py:{serper_api_results_parser.__code__.co_firstlineno}] INFO SerpAPI results parsed for topic '{state.current_resource.topic}' and grade '{state.current_resource.grade}'&quot;)&#10;        return serpapi_search_results&#10;    except Exception as e:&#10;        logging.error(f&quot;[save_to_local.py:{serper_api_results_parser.__code__.co_firstlineno}] ERROR Failed to parse SerpAPI results: {e}&quot;)&#10;        return {}&#10;&#10;def save_to_local(data: Union[dict, list], file_path: str):&#10;    &quot;&quot;&quot;&#10;    Saves the provided data (dict or list) to a local JSON file.&#10;&#10;    Args:&#10;        data (dict or list): The data to be saved.&#10;        file_path (str): The path where the data will be saved.&#10;&#10;    Raises:&#10;        TypeError: If the data contains unsupported types for JSON serialization.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if not isinstance(data, (dict, list)):&#10;            raise TypeError(&quot;Data must be a dict or a list to be saved as JSON.&quot;)&#10;        dir_name = os.path.dirname(file_path)&#10;        if dir_name:&#10;            os.makedirs(dir_name, exist_ok=True)&#10;        with open(file_path, mode='w', encoding='utf-8') as f:&#10;            json.dump(data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;[save_to_local.py:{save_to_local.__code__.co_firstlineno}] INFO Data saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;[save_to_local.py:{save_to_local.__code__.co_firstlineno}] ERROR Failed to save data to {file_path}: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils.py" />
              <option name="updatedContent" value="import os&#10;import json&#10;import logging&#10;&#10;&#10;def save_learning_state_to_json(state, file_path):&#10;    &quot;&quot;&quot;&#10;    Save the details of the LearningState object to a JSON file.&#10;    If the file does not exist, it will be created.&#10;    Args:&#10;        state: LearningState object (should have .model_dump() or .dict() method)&#10;        file_path: Path to the JSON file&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Use model_dump if available (Pydantic v2), else fallback to dict&#10;        if hasattr(state, 'model_dump'):&#10;            state_data = state.model_dump()&#10;        elif hasattr(state, 'dict'):&#10;            state_data = state.dict()&#10;        else:&#10;            raise ValueError(&quot;State object does not support serialization.&quot;)&#10;        # Ensure the directory exists&#10;        os.makedirs(os.path.dirname(file_path), exist_ok=True)&#10;        with open(file_path, 'w', encoding='utf-8') as f:&#10;            json.dump(state_data, f, indent=4, ensure_ascii=False)&#10;        logging.info(f&quot;LearningState saved to {file_path}&quot;)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to save LearningState to {file_path}: {e}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>