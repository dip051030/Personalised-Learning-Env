<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/db/loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/loader.py" />
              <option name="originalContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot; / &quot;lessons&quot;&#10;&#10;&#10;def load_lesson_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    path = DATA_DIR / filename&#10;    logging.info(f&quot;Loading lesson data from {path}&quot;)&#10;    if not path.exists():&#10;        logging.error(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;    logging.info(f&quot;Loaded {len(data)} lessons from {filename}&quot;)&#10;    return data&#10;" />
              <option name="updatedContent" value="import json&#10;from pathlib import Path&#10;from typing import List, Dict, Any&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;DATA_DIR = Path(__file__).parent.parent / &quot;data&quot; / &quot;lessons&quot;&#10;&#10;&#10;def load_lesson_data(filename: str) -&gt; List[Dict[str, Any]]:&#10;    path = DATA_DIR / filename&#10;    logging.info(f&quot;Loading lesson data from {path}&quot;)&#10;    if not path.exists():&#10;        logging.error(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;        raise FileNotFoundError(f&quot;{filename} not found in {DATA_DIR}&quot;)&#10;&#10;    with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        data = json.load(f)&#10;    logging.info(f&quot;Loaded {len(data)} lessons from {filename}&quot;)&#10;    return data" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/db/vector_db.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/db/vector_db.py" />
              <option name="originalContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_lesson_data&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def build_chroma_db_collection(filename: str, collection_name: str = 'lessons'):&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_lesson_data(filename)&#10;    model = SentenceTransformer('all-MiniLM-L6-v2')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;)&#10;        }&#10;        &#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.Client()&#10;    collection = client.create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas=metadatas&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;    return collection, model&#10;" />
              <option name="updatedContent" value="import chromadb&#10;from sentence_transformers import SentenceTransformer&#10;from db.loader import load_lesson_data&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;def build_chroma_db_collection(filename: str, collection_name: str = 'lessons'):&#10;    logging.info(f&quot;Building ChromaDB collection for {filename} with name '{collection_name}'&quot;)&#10;    lessons = load_lesson_data(filename)&#10;    model = SentenceTransformer('all-MiniLM-L6-v2')&#10;    documents = [&#10;        f&quot;{lesson.get('unit', '')} {lesson.get('topic_title', '')} {lesson.get('description', '')}&quot;&#10;        for lesson in lessons&#10;    ]&#10;    logging.info(f&quot;Encoding {len(documents)} documents for embeddings&quot;)&#10;    embeddings = model.encode(documents, show_progress_bar=True).tolist()&#10;    ids = [str(lesson.get('topic_id', i)) for i, lesson in enumerate(lessons)]&#10;    metadatas = [&#10;        {&#10;            &quot;subject&quot;: lesson.get(&quot;subject&quot;),&#10;            &quot;grade&quot;: lesson.get(&quot;grade&quot;),&#10;            &quot;unit&quot;: lesson.get(&quot;unit&quot;),&#10;            &quot;topic_id&quot;: lesson.get(&quot;topic_id&quot;),&#10;            &quot;topic_title&quot;: lesson.get(&quot;topic_title&quot;),&#10;            &quot;keywords&quot;: lesson.get(&quot;keywords&quot;),&#10;            &quot;references&quot;: lesson.get(&quot;references&quot;),&#10;            &quot;hours&quot;: lesson.get(&quot;hours&quot;),&#10;            &quot;type&quot;: lesson.get(&quot;type&quot;)&#10;        }&#10;        &#10;        for lesson in lessons&#10;    ]&#10;&#10;    client = chromadb.Client()&#10;    collection = client.create_collection(name=collection_name)&#10;    logging.info(f&quot;Adding documents and embeddings to ChromaDB collection '{collection_name}'&quot;)&#10;    collection.add(&#10;        documents=documents,&#10;        embeddings=embeddings,&#10;        ids=ids,&#10;        metadatas=metadatas&#10;    )&#10;    logging.info(f&quot;ChromaDB collection '{collection_name}' built successfully&quot;)&#10;    return collection, model" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/keys/apis.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/keys/apis.py" />
              <option name="originalContent" value="import os&#10;import getpass&#10;from typing import Optional&#10;import logging&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;&#10;def set_env(var: str) -&gt; Optional[str]:&#10;    if not os.environ.get(var):&#10;        logging.info(f&quot;Prompting for environment variable: {var}&quot;)&#10;        os.environ[var] = getpass.getpass(f&quot;Enter {var}: &quot;)&#10;    logging.info(f&quot;Environment variable {var} set.&quot;)&#10;    return os.environ.get(var)" />
              <option name="updatedContent" value="import os&#10;import getpass&#10;from typing import Optional&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;&#10;def set_env(var: str) -&gt; Optional[str]:&#10;    if not os.environ.get(var):&#10;        logging.info(f&quot;Prompting for environment variable: {var}&quot;)&#10;        os.environ[var] = getpass.getpass(f&quot;Enter {var}: &quot;)&#10;    logging.info(f&quot;Environment variable {var} set.&quot;)&#10;    return os.environ.get(var)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/logis/logical_functions.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/logis/logical_functions.py" />
              <option name="originalContent" value="import logging&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack&#10;from db.vector_db import build_chroma_db_collection&#10;from sentence_transformers import SentenceTransformer&#10;&#10;def retrieve_and_search(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Retrieve and search for resources based on the current state.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is not None:&#10;            collection, model = build_chroma_db_collection('class_11_physics.json', collection_name='lessons')&#10;            query_embedding = model.encode([state.current_resource.topic]).tolist()&#10;            results = collection.query(&#10;                query_embeddings=query_embedding,&#10;                n_results=1&#10;            )&#10;            return results&#10;    except Exception as e:&#10;        logging.error(f&quot;Error retrieving and searching resources: {e}&quot;)&#10;        return None&#10;&#10;def decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide whether to generate a lesson or a blog.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic&#10;    grade = int(state.user.grade)&#10;    blog_keywords = []&#10;    lesson_keywords = []&#10;&#10;    if any(kw in topic for kw in blog_keywords) and grade &gt; 10:&#10;        return &quot;blog&quot;&#10;    elif any(kw in topic for kw in lesson_keywords):&#10;        return &quot;lesson&quot;&#10;    elif grade &lt;= 10:&#10;        return &quot;lesson&quot;&#10;    else:&#10;        return &quot;lesson&quot;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on user grade and topic.&#10;    &quot;&quot;&quot;&#10;    if state.user.grade &lt;= 6:&#10;        style = &quot;kid_friendly&quot;&#10;    elif state.user.grade &gt;= 10:&#10;        style = &quot;exam_ready&quot;&#10;    elif &quot;practice&quot; in state.current_resource.topic:&#10;        style = &quot;exercise_heavy&quot;&#10;    else:&#10;        style = &quot;general_concept&quot;&#10;    return style&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata[&quot;subject&quot;].lower()),&#10;        grade=metadata[&quot;grade&quot;],&#10;        unit=metadata[&quot;unit&quot;],&#10;        topic_id=metadata[&quot;topic_id&quot;],&#10;        topic=metadata[&quot;topic_title&quot;],&#10;        description=metadata[&quot;description&quot;],&#10;        elaboration=metadata.get(&quot;elaboration&quot;),&#10;        keywords=metadata[&quot;keywords&quot;],&#10;        hours=metadata[&quot;hours&quot;],&#10;        references=metadata[&quot;references&quot;]&#10;    )&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style" />
              <option name="updatedContent" value="import logging&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from schemas import LearningResource, ResourceSubject, LearningState, ContentResponse, FeedBack&#10;from db.vector_db import build_chroma_db_collection&#10;from sentence_transformers import SentenceTransformer&#10;&#10;def retrieve_and_search(state: LearningState) -&gt; dict:&#10;    &quot;&quot;&quot;&#10;    Retrieve and search for resources based on the current state.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        if state.current_resource is not None:&#10;            collection, model = build_chroma_db_collection('class_11_physics.json', collection_name='lessons')&#10;            query_embedding = model.encode([state.current_resource.topic]).tolist()&#10;            results = collection.query(&#10;                query_embeddings=query_embedding,&#10;                n_results=1&#10;            )&#10;            return results&#10;    except Exception as e:&#10;        logging.error(f&quot;Error retrieving and searching resources: {e}&quot;)&#10;        return None&#10;&#10;def decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide whether to generate a lesson or a blog.&#10;    &quot;&quot;&quot;&#10;    topic = state.current_resource.topic&#10;    grade = int(state.user.grade)&#10;    blog_keywords = []&#10;    lesson_keywords = []&#10;&#10;    if any(kw in topic for kw in blog_keywords) and grade &gt; 10:&#10;        return &quot;blog&quot;&#10;    elif any(kw in topic for kw in lesson_keywords):&#10;        return &quot;lesson&quot;&#10;    elif grade &lt;= 10:&#10;        return &quot;lesson&quot;&#10;    else:&#10;        return &quot;lesson&quot;&#10;&#10;def lesson_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide lesson style based on user grade and topic.&#10;    &quot;&quot;&quot;&#10;    if state.user.grade &lt;= 6:&#10;        style = &quot;kid_friendly&quot;&#10;    elif state.user.grade &gt;= 10:&#10;        style = &quot;exam_ready&quot;&#10;    elif &quot;practice&quot; in state.current_resource.topic:&#10;        style = &quot;exercise_heavy&quot;&#10;    else:&#10;        style = &quot;general_concept&quot;&#10;    return style&#10;&#10;def parse_chromadb_metadata(metadata: dict) -&gt; LearningResource:&#10;    &quot;&quot;&quot;&#10;    Convert ChromaDB metadata dict to a LearningResource model.&#10;    &quot;&quot;&quot;&#10;    return LearningResource(&#10;        subject=ResourceSubject(metadata[&quot;subject&quot;].lower()),&#10;        grade=metadata[&quot;grade&quot;],&#10;        unit=metadata[&quot;unit&quot;],&#10;        topic_id=metadata[&quot;topic_id&quot;],&#10;        topic=metadata[&quot;topic_title&quot;],&#10;        description=metadata[&quot;description&quot;],&#10;        elaboration=metadata.get(&quot;elaboration&quot;),&#10;        keywords=metadata[&quot;keywords&quot;],&#10;        hours=metadata[&quot;hours&quot;],&#10;        references=metadata[&quot;references&quot;]&#10;    )&#10;&#10;def blog_decision_node(state: LearningState) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Decide blog style based on topic and user grade.&#10;    &quot;&quot;&quot;&#10;    if &quot;importance&quot; in state.current_resource.topic:&#10;        style = &quot;motivational&quot;&#10;    elif state.user.grade &gt;= 12:&#10;        style = &quot;application_focused&quot;&#10;    else:&#10;        style = &quot;storytelling&quot;&#10;    return style" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="import json&#10;import logging&#10;from nodes import graph_run&#10;&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;user_data = {&#10;    &quot;user&quot;: {&#10;        &quot;username&quot;: &quot;anonymous&quot;,&#10;        &quot;age&quot;: 18,&#10;        &quot;grade&quot;: 19,&#10;        &quot;id&quot;: 1,&#10;        &quot;is_active&quot;: True&#10;    },&#10;    &quot;current_resource&quot;: {&#10;        &quot;subject&quot;: &quot;physics&quot;,&#10;        &quot;grade&quot;: 12,&#10;        &quot;unit&quot;: &quot;Mechanics&quot;,&#10;        &quot;topic_id&quot;: '',&#10;        &quot;topic&quot;: &quot;period of pendulum&quot;,&#10;        &quot;description&quot;: &quot;&quot;,&#10;        &quot;elaboration&quot;: &quot;&quot;,&#10;        &quot;keywords&quot;: [],&#10;        &quot;hours&quot;: 1,&#10;        &quot;references&quot;: &quot;&quot;&#10;    },&#10;    &quot;progress&quot;: [],&#10;    &quot;next_action&quot;: &quot;select_resource&quot;,&#10;    &quot;history&quot;: []&#10;}&#10;&#10;output = graph_run(user_data)&#10;logging.info(f&quot;Graph output: {output}&quot;)" />
              <option name="updatedContent" value="import json&#10;import logging&#10;from nodes import graph_run&#10;&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;user_data = {&#10;    &quot;user&quot;: {&#10;        &quot;username&quot;: &quot;anonymous&quot;,&#10;        &quot;age&quot;: 18,&#10;        &quot;grade&quot;: 19,&#10;        &quot;id&quot;: 1,&#10;        &quot;is_active&quot;: True&#10;    },&#10;    &quot;current_resource&quot;: {&#10;        &quot;subject&quot;: &quot;physics&quot;,&#10;        &quot;grade&quot;: 12,&#10;        &quot;unit&quot;: &quot;Mechanics&quot;,&#10;        &quot;topic_id&quot;: '',&#10;        &quot;topic&quot;: &quot;period of pendulum&quot;,&#10;        &quot;description&quot;: &quot;&quot;,&#10;        &quot;elaboration&quot;: &quot;&quot;,&#10;        &quot;keywords&quot;: [],&#10;        &quot;hours&quot;: 1,&#10;        &quot;references&quot;: &quot;&quot;&#10;    },&#10;    &quot;progress&quot;: [],&#10;    &quot;next_action&quot;: &quot;select_resource&quot;,&#10;    &quot;history&quot;: []&#10;}&#10;&#10;output = graph_run(user_data)&#10;logging.info(f&quot;Graph output: {output}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/llm_models.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/llm_models.py" />
              <option name="originalContent" value="import logging&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;from langchain_google_genai import ChatGoogleGenerativeAI&#10;from langchain_openai import ChatOpenAI&#10;from langchain_groq import  ChatGroq&#10;from keys.apis import set_env&#10;from schemas import UserInfo&#10;&#10;def get_gemini_model(output_schema):&#10;    logging.info(&quot;Initializing Gemini model with structured output.&quot;)&#10;    return ChatGoogleGenerativeAI(&#10;        model='gemini-2.0-flash',&#10;        api_key=set_env('GOOGLE_API_KEY'),&#10;        temperature=1,&#10;    ).with_structured_output(output_schema)&#10;&#10;def get_groq_model():&#10;    logging.info(&quot;Initializing Groq model.&quot;)&#10;    return ChatGroq(&#10;        model='meta-llama/llama-4-scout-17b-16e-instruct',&#10;        api_key=set_env('GROQ_API_KEY'),&#10;        temperature=0.5&#10;    )&#10;&#10;def get_openai_model():&#10;    logging.info(&quot;Initializing OpenAI model.&quot;)&#10;    return ChatOpenAI(&#10;        model='gpt-4o',&#10;        temperature=0.5,&#10;        api_key=set_env('OPENAI_API_KEY')&#10;    )" />
              <option name="updatedContent" value="import logging&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;from langchain_google_genai import ChatGoogleGenerativeAI&#10;from langchain_openai import ChatOpenAI&#10;from langchain_groq import  ChatGroq&#10;from keys.apis import set_env&#10;from schemas import UserInfo&#10;&#10;def get_gemini_model(output_schema):&#10;    logging.info(&quot;Initializing Gemini model with structured output.&quot;)&#10;    return ChatGoogleGenerativeAI(&#10;        model='gemini-2.0-flash',&#10;        api_key=set_env('GOOGLE_API_KEY'),&#10;        temperature=1,&#10;    ).with_structured_output(output_schema)&#10;&#10;def get_groq_model():&#10;    logging.info(&quot;Initializing Groq model.&quot;)&#10;    return ChatGroq(&#10;        model='meta-llama/llama-4-scout-17b-16e-instruct',&#10;        api_key=set_env('GROQ_API_KEY'),&#10;        temperature=0.5&#10;    )&#10;&#10;def get_openai_model():&#10;    logging.info(&quot;Initializing OpenAI model.&quot;)&#10;    return ChatOpenAI(&#10;        model='gpt-4o',&#10;        temperature=0.5,&#10;        api_key=set_env('OPENAI_API_KEY')&#10;    )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/nodes.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/nodes.py" />
              <option name="originalContent" value="from langchain_core.messages import SystemMessage, HumanMessage&#10;from langgraph.graph import StateGraph, START, END&#10;from langsmith import expect&#10;from pydantic_core import ValidationError&#10;from sympy.stats import Expectation&#10;import logging&#10;&#10;from logis.logical_functions import decision_node, lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation, \&#10;    CONTENT_FEEDBACK_SYSTEM_PROMPT&#10;from schemas import LearningState, ContentResponse&#10;import  json&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering user_info_node&quot;)&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering enrich_content node&quot;)&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved_content = retrieve_and_search(state=state)&#10;            logging.info(f&quot;Retrieved content: {retrieved_content}&quot;)&#10;            response= enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.current_resource = state.current_resource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.current_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering route_selector_node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route for resource: {state.current_resource}&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources' : state.current_resource.model_dump()&#10;            })&#10;            state.content_type = ContentResponse.LESSON if decision_node(state) == &quot;lesson_selection&quot; else ContentResponse.BLOG&#10;            state.next_action = response.content if hasattr(response, &quot;content&quot;) else response&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_lesson_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;&#10;            state.content.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;            logging.info(f&quot;Lesson content has been generated!&quot;)&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_blog_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        pass&#10;    try:&#10;        logical_response = blog_decision_node(state=state)&#10;        logging.info(f&quot;Logical response for blog generation: {logical_response}&quot;)&#10;        response = blog_generation.invoke({&#10;            &quot;action&quot;: &quot;generate_lesson&quot;,&#10;            &quot;user_data&quot;: state.user.model_dump(),&#10;            &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;            &quot;style&quot;: logical_response&#10;        })&#10;&#10;        state.content.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;        logging.info(f&quot;Blog content has been generated!&quot;)&#10;    except Expectation as e:&#10;        logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering content_improviser_node&quot;)&#10;    if state.content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_IMPROVISE_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.model_dump()}&#10;&quot;&quot;&quot;)&#10;            ]&#10;&#10;            response = content_improviser(messages)&#10;            generated_markdown = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            state.content.content = ContentResponse(content=generated_markdown)&#10;            logging.info(f&quot;Improvised content has been generated!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;def collect_feedback_node(state:LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering collect_feedback_node&quot;)&#10;    if state.content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_FEEDBACK_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Feedback:&#10;{state.content.content}&#10;&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = messages&#10;            logging.info(f&quot;Collecting feedback for content: {state.content.content}&quot;)&#10;            logging.info(f&quot;Response: {response}&quot;)&#10;            # Assume feedback is collected and processed&#10;        except Exception as e:&#10;            logging.error(f&quot;Error collecting feedback: {e}&quot;)&#10;    return state&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: &quot;blog_generation&quot; if state.next_action == &quot;blog&quot; else &quot;content_generation&quot;,&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, END)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))&#10;" />
              <option name="updatedContent" value="from langchain_core.messages import SystemMessage, HumanMessage&#10;from langgraph.graph import StateGraph, START, END&#10;from langsmith import expect&#10;from pydantic_core import ValidationError&#10;from sympy.stats import Expectation&#10;import logging&#10;&#10;from logis.logical_functions import decision_node, lesson_decision_node, blog_decision_node, parse_chromadb_metadata, \&#10;    retrieve_and_search&#10;from prompts.prompts import user_summary, enriched_content, \&#10;    content_improviser, CONTENT_IMPROVISE_SYSTEM_PROMPT, route_selector, blog_generation, content_generation, \&#10;    CONTENT_FEEDBACK_SYSTEM_PROMPT&#10;from schemas import LearningState, ContentResponse&#10;import  json&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;&#10;def user_info_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering user_info_node&quot;)&#10;    if state.user is not None:&#10;        try:&#10;            response = user_summary.invoke({&#10;                &quot;action&quot;: &quot;summarise_user&quot;,&#10;                &quot;existing_data&quot;: state.user.model_dump()&#10;            })&#10;&#10;            user_data = response.content if hasattr(response, 'content') else response&#10;            state.user = state.user.model_validate(user_data if isinstance(user_data, dict) else user_data.model_dump())&#10;            logging.info(f&quot;User info processed: {state.user}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing user data: {e}&quot;)&#10;    return state&#10;&#10;&#10;def enrich_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering enrich_content node&quot;)&#10;    if state.current_resource is not None:&#10;        try:&#10;            retrieved_content = retrieve_and_search(state=state)&#10;            logging.info(f&quot;Retrieved content: {retrieved_content}&quot;)&#10;            response= enriched_content.invoke({&#10;                &quot;action&quot;: &quot;content_enrichment&quot;,&#10;                &quot;current_resources_data&quot;: parse_chromadb_metadata(retrieved_content).model_dump()&#10;            })&#10;&#10;            resource_data = response.content if hasattr(response, &quot;content&quot;) else response&#10;            state.current_resource = state.current_resource.model_validate(resource_data)&#10;            logging.info(f&quot;Learning resource processed: {state.current_resource}&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error processing learning resource data: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;&#10;def route_selector_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering route_selector_node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logging.info(f&quot;Selecting the route for resource: {state.current_resource}&quot;)&#10;            response = route_selector.invoke({&#10;                'current_resources' : state.current_resource.model_dump()&#10;            })&#10;            state.content_type = ContentResponse.LESSON if decision_node(state) == &quot;lesson_selection&quot; else ContentResponse.BLOG&#10;            state.next_action = response.content if hasattr(response, &quot;content&quot;) else response&#10;            logging.info(f&quot;Route selection response: {state.next_action}&quot;)&#10;&#10;        except Exception as e:&#10;            logging.error(f&quot;Error selecting route: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_lesson_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_lesson_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        try:&#10;            logical_response = lesson_decision_node(state=state)&#10;            logging.info(f&quot;Logical response for lesson generation: {logical_response}&quot;)&#10;            response = content_generation.invoke({&#10;                &quot;action&quot;: &quot;generate_lesson&quot;,&#10;                &quot;user_data&quot;: state.user.model_dump(),&#10;                &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;                &quot;style&quot;: logical_response&#10;            })&#10;&#10;&#10;            state.content.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;            logging.info(f&quot;Lesson content has been generated!&quot;)&#10;        except Expectation as e:&#10;            logging.error(f&quot;Error generating lesson content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def generate_blog_content(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering generate_blog_content node&quot;)&#10;    if state.user is not None and state.current_resource is not None:&#10;        pass&#10;    try:&#10;        logical_response = blog_decision_node(state=state)&#10;        logging.info(f&quot;Logical response for blog generation: {logical_response}&quot;)&#10;        response = blog_generation.invoke({&#10;            &quot;action&quot;: &quot;generate_lesson&quot;,&#10;            &quot;user_data&quot;: state.user.model_dump(),&#10;            &quot;resource_data&quot;: state.current_resource.model_dump(),&#10;            &quot;style&quot;: logical_response&#10;        })&#10;&#10;        state.content.content = ContentResponse(content=response.content if hasattr(response, &quot;content&quot;) else response)&#10;        logging.info(f&quot;Blog content has been generated!&quot;)&#10;    except Expectation as e:&#10;        logging.error(f&quot;Error generating blog content: {e}&quot;)&#10;    return state&#10;&#10;&#10;def content_improviser_node(state: LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering content_improviser_node&quot;)&#10;    if state.content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_IMPROVISE_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Unpolished Learning Resource:&#10;{state.content.model_dump()}&#10;&quot;&quot;&quot;)&#10;            ]&#10;&#10;            response = content_improviser(messages)&#10;            generated_markdown = response.content if hasattr(response, &quot;content&quot;) else str(response)&#10;            state.content.content = ContentResponse(content=generated_markdown)&#10;            logging.info(f&quot;Improvised content has been generated!&quot;)&#10;        except Exception as e:&#10;            logging.error(f&quot;Error improvising content: {e}&quot;)&#10;&#10;    return state&#10;&#10;&#10;def collect_feedback_node(state:LearningState) -&gt; LearningState:&#10;    logging.info(&quot;Entering collect_feedback_node&quot;)&#10;    if state.content is not None:&#10;        try:&#10;            messages = [&#10;                CONTENT_FEEDBACK_SYSTEM_PROMPT,&#10;                HumanMessage(content=f&quot;&quot;&quot;&#10;Feedback:&#10;{state.content.content}&#10;&#10;&quot;&quot;&quot;)&#10;            ]&#10;            response = messages&#10;            logging.info(f&quot;Collecting feedback for content: {state.content.content}&quot;)&#10;            logging.info(f&quot;Response: {response}&quot;)&#10;            # Assume feedback is collected and processed&#10;        except Exception as e:&#10;            logging.error(f&quot;Error collecting feedback: {e}&quot;)&#10;    return state&#10;&#10;builder = StateGraph(LearningState)&#10;builder.add_node(&quot;user_info&quot;, user_info_node)&#10;builder.add_node(&quot;learning_resource&quot;, enrich_content)&#10;builder.add_node(&quot;route_selector&quot;, route_selector_node)&#10;builder.add_node(&quot;content_generation&quot;, generate_lesson_content)&#10;builder.add_node(&quot;blog_generation&quot;, generate_blog_content)&#10;builder.add_node(&quot;content_improviser&quot;, content_improviser_node)&#10;&#10;builder.set_entry_point(&quot;user_info&quot;)&#10;builder.add_edge(&quot;user_info&quot;, &quot;learning_resource&quot;)&#10;builder.add_edge(&quot;learning_resource&quot;, &quot;route_selector&quot;)&#10;builder.add_conditional_edges(&#10;    &quot;route_selector&quot;,&#10;    lambda state: &quot;blog_generation&quot; if state.next_action == &quot;blog&quot; else &quot;content_generation&quot;,&#10;    {&#10;        &quot;blog_generation&quot;: &quot;blog_generation&quot;,&#10;        &quot;content_generation&quot;: &quot;content_generation&quot;&#10;    }&#10;)&#10;builder.add_edge(&quot;content_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;blog_generation&quot;, &quot;content_improviser&quot;)&#10;builder.add_edge(&quot;content_improviser&quot;, END)&#10;&#10;graph = builder.compile()&#10;&#10;def graph_run(user_data: dict):&#10;    return graph.invoke(LearningState.model_validate(user_data))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/prompts/prompts.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/prompts/prompts.py" />
              <option name="originalContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;import logging&#10;&#10;logging.basicConfig(level=logging.INFO)&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse&#10;# from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing UserSummaryTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data:&#10;{existing_data}&#10;&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(f&quot;Formatting UserSummaryTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    A simplified prompt template for enriching a learning resource.&#10;    The model is expected to:&#10;    - Expand and clarify fields like 'description' and 'elaboration'&#10;    - Generate summaries and student-friendly explanations&#10;    - Maintain the same keys as input (structured JSON output)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing EnrichContent Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a curriculum enrichment agent. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields (like `description`, `elaboration`)&#10;- Add a student-friendly summary&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;current_resources_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resource_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Converts the provided topic data dictionary into a JSON-formatted&#10;        string to insert into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(&quot;Formatting EnrichContent prompt&quot;)&#10;        return self.format(&#10;            current_resource_data=json.dumps(current_resource_data, indent=2)&#10;        )&#10;&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;&#10;    The model must return ONLY the markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing ContentGenerationTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and dont introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting ContentGenerationTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;&#10;from langchain.prompts import PromptTemplate&#10;import json&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing BlogGenerationPrompt Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting BlogGenerationPrompt for style: {style}&quot;)&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing RouteSelectorNode Template&quot;)&#10;        super().__init__(&#10;            template = '''&#10;            You are a route selector for an educational learning system.&#10;            Your task is to determine the next action based on the user''s current state and progress.&#10;            Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, current_resources: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting RouteSelectorNode prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_FEEDBACK_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an intelligent feedback assistant trained to process and structure user feedback on educational content.&#10;&#10;Your goal is to analyze provided content and  comments and generate a clean JSON object with the following fields:&#10;&#10;- `resource`: A generated content.&#10;- `rating`: An integer from 1 to 5 (1 = very poor, 5 = excellent).&#10;- `comments`: A short summary of the feedback if available.&#10;&#10;You must only return valid JSON — do not include any extra text or explanation. Assume the input may include both praise and criticism, and extract the most relevant sentiment into the structure.&#10;&#10;You never invent new fields. You do not explain your reasoning.&#10;&quot;&quot;&quot;)&#10;&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(LearningResource)&#10;# user_content_generation = prompt_content_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichContent)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelectorNode)&#10;content_generation = prompt_content_generation | get_gemini_model(LearningResource)&#10;blog_generation = prompt_blog_generation | get_gemini_model(LearningResource)&#10;content_improviser = get_groq_model()" />
              <option name="updatedContent" value="from langchain_core.messages import SystemMessage&#10;from langchain_core.prompts import PromptTemplate&#10;import json&#10;import logging&#10;&#10;logging.basicConfig(&#10;    level=logging.INFO,&#10;    format='%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s',&#10;    datefmt='%Y-%m-%d %H:%M:%S'&#10;)&#10;&#10;from pydantic import BaseModel&#10;&#10;from models.llm_models import get_gemini_model, get_groq_model&#10;from schemas import UserInfo, LearningResource, LearningState, ContentResponse&#10;# from nodes import user_info_node, learning_resource_node, content_generation, content_improviser_node&#10;&#10;&#10;# ---------------------------------------------------------------------------------&#10;#  UserSummaryTemplate: Prompt to turn raw user data into natural-language JSON&#10;# ---------------------------------------------------------------------------------&#10;&#10;class UserSummaryTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for summarizing user data. The LLM is instructed to return natural&#10;    language descriptions for **each key** in the user object, including fields&#10;    like `username`, `age`, `grade`, `is_active`, `subject`, and `topic`.&#10;&#10;    It returns a JSON object with the same keys, but values are reworded explanations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing UserSummaryTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;Your role is {action}. You are given structured user data:&#10;{existing_data}&#10;&#10;Your task:&#10;- For **every key-value pair**, write a meaningful, human-readable summary.&#10;- Maintain the same keys in the output.&#10;- Summarize any field, including: `username`, `age`, `grade`, `is_active` or any others present.&#10;&#10;Return the result as a JSON formatted object only. Do **not** add explanations outside of the JSON.&#10;&#10;Example output:&#10;{{&#10;  &quot;username&quot;: &quot;User's name is dyane_master&quot;,&#10;  &quot;age&quot;: &quot;User is 22 years old&quot;,&#10;  &quot;grade&quot;: &quot;User is halfway through Grade 12&quot;,&#10;  'id': &quot;User's ID is 1&quot;,&#10;  &quot;is_active&quot;: &quot;User is currently active&quot;,&#10;  &quot;user_info&quot;: &quot;The user named dyane_master is 22 years old and is currently active. They are halfway through Grade 12 and currently focused on academic growth. No further user details are available at this time.&quot;&#10;}}&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;existing_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, existing_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Convert the input dict to a pretty JSON string for better formatting and inject it into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(f&quot;Formatting UserSummaryTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            existing_data=json.dumps(existing_data, indent=2)&#10;        )&#10;&#10;# -----------------------------------------------------------------------------------------&#10;#  LearningResourceTemplate: Summarizes learning content + links it to user interest&#10;# -----------------------------------------------------------------------------------------&#10;&#10;class EnrichContent(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    A simplified prompt template for enriching a learning resource.&#10;    The model is expected to:&#10;    - Expand and clarify fields like 'description' and 'elaboration'&#10;    - Generate summaries and student-friendly explanations&#10;    - Maintain the same keys as input (structured JSON output)&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing EnrichContent Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a curriculum enrichment agent. Based on this structured topic:&#10;{current_resources_data}&#10;&#10;Your task:&#10;- Enrich vague or brief fields (like `description`, `elaboration`)&#10;- Add a student-friendly summary&#10;- Include optional insights if relevant (e.g., practical uses, visual analogies)&#10;- Keep original keys. Maintain consistent structure.&#10;&#10;Return only a single valid JSON object. Do not explain your process.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;current_resources_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, current_resource_data: dict) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Converts the provided topic data dictionary into a JSON-formatted&#10;        string to insert into the prompt.&#10;        &quot;&quot;&quot;&#10;        logging.info(&quot;Formatting EnrichContent prompt&quot;)&#10;        return self.format(&#10;            current_resource_data=json.dumps(current_resource_data, indent=2)&#10;        )&#10;&#10;&#10;class ContentGenerationTemplate(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Prompt to generate markdown educational content ONLY.&#10;&#10;    The model must return ONLY the markdown content as a plain string.&#10;    No JSON, no metadata, no explanations outside the content.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing ContentGenerationTemplate&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You are an educational content generator.&#10;&#10;Task: {action}&#10;&#10;User Info:&#10;{user_data}&#10;&#10;Learning Resource:&#10;{resource_data}&#10;&#10;Instructions:&#10;- Generate a clear, structured markdown lesson/explanation.&#10;- Explain concepts in a way that feels like a friendly tutor.&#10;- Focus on the subject and topic provided.&#10;- Use headings, bullet points, and code blocks as needed.&#10;- Ensure the content is educational and engaging.&#10;- Tailor it to the user's grade level and interests.&#10;- Return ONLY the markdown content text and dont introduce yourself or provide any other information.&#10;- Do NOT return JSON, metadata, or extra commentary.&#10;&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;action&quot;, &quot;user_data&quot;, &quot;resource_data&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, user_data: dict, resource_data: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting ContentGenerationTemplate prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_IMPROVISE_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an energetic and insightful educational content improver and enhancer.&#10;&#10;Your task:  &#10;Take the given educational content and improve it by making it more engaging, clear, and reader-friendly while preserving the original meaning and key points.&#10;&#10;Focus on:  &#10;- Enhancing structure with clear markdown headings, bullet points, and examples.  &#10;- Injecting a warm, professional, and approachable tone — friendly but not overly casual.  &#10;- Adding vivid metaphors and real-world connections to make concepts memorable.  &#10;- Improving flow and readability — make it easy to scan and digest.  &#10;- Including occasional motivational nudges or thoughtful questions (1-2 per passage) that invite reflection and curiosity without overwhelming the reader.  &#10;- Avoiding unnecessary repetition or filler language.  &#10;- Explaining *why* topics matter, not just *what* they are.  &#10;- Maintaining concise, clear language suitable for motivated learners who want efficient and deep understanding.  &#10;&#10;**Important:**  &#10;- Return ONLY the markdown content.  &#10;- DO NOT return JSON, metadata, or any extra explanations.  &#10;&#10;Example opening you might use to improve a draft:  &#10;“Let’s dive into [subject] — understanding this will unlock powerful tools for your learning journey!”&#10;&#10;Now, improve the following content:&#10;&#10;&quot;&quot;&quot;)&#10;&#10;&#10;from langchain.prompts import PromptTemplate&#10;import json&#10;&#10;class BlogGenerationPrompt(PromptTemplate):&#10;    &quot;&quot;&quot;&#10;    Template for generating educational blog posts.&#10;    The LLM should:&#10;    - Translate academic topic into engaging blog format&#10;    - Make it informative but also fun/relatable&#10;    - Consider user's interests and the style logic&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing BlogGenerationPrompt Template&quot;)&#10;        super().__init__(&#10;            template=(&#10;                &quot;&quot;&quot;You're a friendly education blogger.&#10;&#10;USER PROFILE:&#10;{user_data}&#10;&#10;TOPIC INFORMATION:&#10;{resource_data}&#10;&#10;STYLE TO FOLLOW:&#10;{style}&#10;&#10;Write a short, engaging blog post for students based on the above topic.&#10;&#10;- Make it informative but not too formal.&#10;- Use real-world analogies and visuals if appropriate.&#10;- Output Markdown-formatted blog content only.&quot;&quot;&quot;&#10;            ),&#10;            input_variables=[&quot;user_data&quot;, &quot;resource_data&quot;, &quot;style&quot;]&#10;        )&#10;&#10;    def format_prompt(self, user_data: dict, resource_data: dict, style: str) -&gt; str:&#10;        logging.info(f&quot;Formatting BlogGenerationPrompt for style: {style}&quot;)&#10;        return self.format(&#10;            user_data=json.dumps(user_data, indent=2),&#10;            resource_data=json.dumps(resource_data, indent=2),&#10;            style=style&#10;        )&#10;&#10;class RouteSelectorNode(PromptTemplate):&#10;    def __init__(self):&#10;        logging.info(&quot;Initializing RouteSelectorNode Template&quot;)&#10;        super().__init__(&#10;            template = '''&#10;            You are a route selector for an educational learning system.&#10;            Your task is to determine the next action based on the user''s current state and progress.&#10;            Based on the {current_resources} decide whether to generate a blog or a lesson and return the output.'''&#10;    , input_variables=[&quot;current_resources&quot;]&#10;        )&#10;&#10;    def format_prompt(self, action: str, current_resources: dict) -&gt; str:&#10;        logging.info(f&quot;Formatting RouteSelectorNode prompt for action: {action}&quot;)&#10;        return self.format(&#10;            action=action,&#10;            current_resources=json.dumps(current_resources, indent=2)&#10;        )&#10;&#10;&#10;CONTENT_FEEDBACK_SYSTEM_PROMPT = SystemMessage(content=&quot;&quot;&quot;&#10;You are an intelligent feedback assistant trained to process and structure user feedback on educational content.&#10;&#10;Your goal is to analyze provided content and  comments and generate a clean JSON object with the following fields:&#10;&#10;- `resource`: A generated content.&#10;- `rating`: An integer from 1 to 5 (1 = very poor, 5 = excellent).&#10;- `comments`: A short summary of the feedback if available.&#10;&#10;You must only return valid JSON — do not include any extra text or explanation. Assume the input may include both praise and criticism, and extract the most relevant sentiment into the structure.&#10;&#10;You never invent new fields. You do not explain your reasoning.&#10;&quot;&quot;&quot;)&#10;&#10;&#10;prompt_user = UserSummaryTemplate()&#10;prompt_enrichment = EnrichContent()&#10;prompt_content_generation = ContentGenerationTemplate()&#10;prompt_content_improviser = CONTENT_IMPROVISE_SYSTEM_PROMPT&#10;prompt_route_selector = RouteSelectorNode()&#10;prompt_blog_generation = BlogGenerationPrompt()&#10;&#10;user_summary = prompt_user | get_gemini_model(LearningResource)&#10;# user_content_generation = prompt_content_model(UserInfo)&#10;enriched_content = prompt_enrichment | get_gemini_model(EnrichContent)&#10;route_selector = prompt_route_selector | get_gemini_model(RouteSelectorNode)&#10;content_generation = prompt_content_generation | get_gemini_model(LearningResource)&#10;blog_generation = prompt_blog_generation | get_gemini_model(LearningResource)&#10;content_improviser = get_groq_model()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>